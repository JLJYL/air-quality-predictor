# =================================================================
# å°å…¥æ‰€æœ‰å¿…è¦çš„åº«
# =================================================================
import requests
import pandas as pd
import datetime
import random
import re
import os
import warnings
import numpy as np
import xgboost as xgb
from datetime import timedelta, timezone
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from meteostat import Point, Hourly, units
from flask import Flask, render_template

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# =================================================================
# å…¨åŸŸè®Šæ•¸ - åƒ…åœ¨æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•æ™‚è¨­å®šä¸€æ¬¡
# =================================================================
TRAINED_MODELS = {} 
LAST_OBSERVATION = None 
FEATURE_COLUMNS = []
POLLUTANT_PARAMS = [] # å¯¦éš›æ‰¾åˆ°ä¸¦è¨“ç·´çš„æ¨¡å‹åƒæ•¸
HOURS_TO_PREDICT = 24

# =================================================================
# å¸¸æ•¸è¨­å®š (æ¥µé™å„ªåŒ–å€åŸŸ)
# =================================================================
API_KEY = "68af34aea77a19aa1137ee5fd9b287229ccf23a686309b4521924a04963ac663"
API_BASE_URL = "https://api.openaq.org/v3/"
POLLUTANT_TARGETS = ["pm25", "pm10", "o3", "no2", "so2", "co"]
LOCAL_TZ = "Asia/Taipei"
MIN_DATA_THRESHOLD = 50 
LAG_HOURS = [1, 2, 3, 6, 12] # ä¿ç•™åŸºæœ¬æ»¯å¾Œç‰¹å¾µ
# ROLLING_WINDOWS = [6, 12] # <-- åˆªé™¤æ»¾å‹•çª—å£ç‰¹å¾µä»¥é™ä½è¨ˆç®—è¤‡é›œåº¦
DAYS_TO_FETCH = 2 # <<-- é—œéµèª¿æ•´ï¼šå¾ 3 å¤©æ¸›å°‘åˆ° 2 å¤© (æ•¸æ“šé‡æœ€å°åŒ–)

# æ¨¡å‹è¨“ç·´åƒæ•¸ï¼šæ¥µé™å„ªåŒ–é€Ÿåº¦
N_ESTIMATORS = 20 # <<-- é—œéµèª¿æ•´ï¼šå¾ 40 æ¸›å°‘åˆ° 20 (è¨“ç·´æ™‚é–“æœ€å°åŒ–)
MAX_DEPTH = 5 # <<-- æ–°å¢èª¿æ•´ï¼šå¾ 7 æ¸›å°‘åˆ° 5 (æ¨¡å‹æ·±åº¦æœ€å°åŒ–)

# ç°¡åŒ–çš„ AQI åˆ†ç´šè¡¨ (åŸºæ–¼å°æ™‚å€¼å’Œ US EPA æ¨™æº–çš„å¸¸ç”¨æ•¸å€¼)
AQI_BREAKPOINTS = {
Â  Â  "pm25": [(0.0, 12.0, 0, 50), (12.1, 35.4, 51, 100), (35.5, 55.4, 101, 150), (55.5, 150.4, 151, 200)],
Â  Â  "pm10": [(0, 54, 0, 50), (55, 154, 51, 100), (155, 254, 101, 150), (255, 354, 151, 200)],
Â  Â  "o3": [(0, 54, 0, 50), (55, 70, 51, 100), (71, 85, 101, 150), (86, 105, 151, 200)],
Â  Â  "co": [(0.0, 4.4, 0, 50), (4.5, 9.4, 51, 100), (9.5, 12.4, 101, 150), (12.5, 15.4, 151, 200)],
Â  Â  "no2": [(0, 100, 0, 50), (101, 360, 51, 100), (361, 649, 101, 150), (650, 1249, 151, 200)],
Â  Â  "so2": [(0, 35, 0, 50), (36, 75, 51, 100), (76, 185, 101, 150), (186, 304, 151, 200)],
}

# =================================================================
# è¼”åŠ©å‡½å¼: AQI è¨ˆç®— (æœªä¿®æ”¹)
# =================================================================

def calculate_aqi_sub_index(param: str, concentration: float) -> float:
Â  Â  """è¨ˆç®—å–®ä¸€æ±¡æŸ“ç‰©æ¿ƒåº¦å°æ‡‰çš„ AQI å­æŒ‡æ•¸ (I)"""
Â  Â  if pd.isna(concentration) or concentration < 0:
Â  Â  Â  Â  return 0

Â  Â  breakpoints = AQI_BREAKPOINTS.get(param)
Â  Â  if not breakpoints:
Â  Â  Â  Â  return 0

Â  Â  for C_low, C_high, I_low, I_high in breakpoints:
Â  Â  Â  Â  if C_low <= concentration <= C_high:
Â  Â  Â  Â  Â  Â  if C_high == C_low:
Â  Â  Â  Â  Â  Â  Â  Â  return I_high
Â  Â  Â  Â  Â  Â  I = ((I_high - I_low) / (C_high - C_low)) * (concentration - C_low) + I_low
Â  Â  Â  Â  Â  Â  return np.round(I)

Â  Â  Â  Â  if concentration > breakpoints[-1][1]:
Â  Â  Â  Â  Â  Â  I_low, I_high = breakpoints[-1][2], breakpoints[-1][3]
Â  Â  Â  Â  Â  Â  C_low, C_high = breakpoints[-1][0], breakpoints[-1][1]
Â  Â  Â  Â  Â  Â  if C_high == C_low:
Â  Â  Â  Â  Â  Â  Â  Â  return I_high
Â  Â  Â  Â  Â  Â  I_rate = (I_high - I_low) / (C_high - C_low)
Â  Â  Â  Â  Â  Â  I = I_high + I_rate * (concentration - C_high)
Â  Â  Â  Â  Â  Â  return np.round(I)

Â  Â  return 0

def calculate_aqi(row: pd.Series, params: list) -> int:
Â  Â  """æ ¹æ“šå¤šå€‹æ±¡æŸ“ç‰©æ¿ƒåº¦è¨ˆç®—æœ€çµ‚ AQI (å–æœ€å¤§å­æŒ‡æ•¸)"""
Â  Â  sub_indices = []
Â  Â  for p in params:
Â  Â  Â  Â  col_name = f'{p}_pred' if f'{p}_pred' in row else f'{p}_value'
Â  Â  Â  Â  if col_name in row and not pd.isna(row[col_name]):
Â  Â  Â  Â  Â  Â  sub_index = calculate_aqi_sub_index(p, row[col_name])
Â  Â  Â  Â  Â  Â  sub_indices.append(sub_index)

Â  Â  if not sub_indices:
Â  Â  Â  Â  return np.nan

Â  Â  return int(np.max(sub_indices))

# =================================================================
# OpenAQ V3 æ•¸æ“šçˆ¬å–/è¼”åŠ©å‡½å¼ (æœªä¿®æ”¹)
# =================================================================
def sanitize_filename(name: str) -> str:
Â  Â  return re.sub(r'[\\/:"*?<>|]+', '_', name)

def get_nearest_station(lat, lon, radius=20000, limit=50, days=7):
Â  Â  """ æ‰¾é›¢ (lat,lon) æœ€è¿‘ä¸”æœ€è¿‘ days å…§æœ‰æ›´æ–°çš„æ¸¬ç«™ """
Â  Â  url = f"{API_BASE_URL}locations"
Â  Â  headers = {"X-API-Key": API_KEY}
Â  Â  params = {"coordinates": f"{lat},{lon}", "radius": radius, "limit": limit}
Â  Â  try:
Â  Â  Â  Â  resp = requests.get(url, headers=headers, params=params)
Â  Â  Â  Â  resp.raise_for_status()
Â  Â  Â  Â  j = resp.json()
Â  Â  except Exception as e:
Â  Â  Â  Â  print(f"Error fetching nearest station: {e}")
Â  Â  Â  Â  return None

Â  Â  if "results" not in j or not j["results"]:
Â  Â  Â  Â  return None

Â  Â  df = pd.json_normalize(j["results"])
Â  Â  if "datetimeLast.utc" not in df.columns:
Â  Â  Â  Â  return None

Â  Â  df["datetimeLast.utc"] = pd.to_datetime(df["datetimeLast.utc"], errors="coerce", utc=True)
Â  Â  now = pd.Timestamp.utcnow()
Â  Â  cutoff = now - pd.Timedelta(days=days)
Â  Â  df = df[(df["datetimeLast.utc"] >= cutoff) & (df["datetimeLast.utc"] <= now)]
Â  Â  if df.empty:
Â  Â  Â  Â  return None

Â  Â  nearest = df.sort_values("distance").iloc[0]
Â  Â  return nearest.to_dict()

def get_station_sensors(station_id):
Â  Â  """ ä½¿ç”¨ /locations/{id}/sensors å–å¾— sensors åˆ—è¡¨ """
Â  Â  url = f"{API_BASE_URL}locations/{station_id}/sensors"
Â  Â  headers = {"X-API-Key": API_KEY}
Â  Â  try:
Â  Â  Â  Â  resp = requests.get(url, headers=headers, params={"limit":1000})
Â  Â  Â  Â  resp.raise_for_status()
Â  Â  Â  Â  j = resp.json()
Â  Â  Â  Â  return j.get("results", [])
Â  Â  except Exception as e:
Â  Â  Â  Â  print(f"Error fetching sensors: {e}")
Â  Â  Â  Â  return []

def _extract_datetime_from_measurement(item: dict):
Â  Â  """ å˜—è©¦å¾ measurement ç‰©ä»¶æŠ½å‡ºæ™‚é–“å­—ä¸² """
Â  Â  candidates = [("period", "datetimeFrom", "utc"), ("date", "utc"), ("datetime",)]
Â  Â  for path in candidates:
Â  Â  Â  Â  cur = item
Â  Â  Â  Â  ok = True
Â  Â  Â  Â  for k in path:
Â  Â  Â  Â  Â  Â  if isinstance(cur, dict) and k in cur:
Â  Â  Â  Â  Â  Â  Â  Â  cur = cur[k]
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  ok = False
Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  if ok and cur:
Â  Â  Â  Â  Â  Â  return cur
Â  Â  return None

def fetch_sensor_data(sensor_id, param_name, limit=500, days=7):
Â  Â  """ æ“·å– sensor çš„æ™‚é–“åºåˆ— """
Â  Â  url = f"{API_BASE_URL}sensors/{sensor_id}/measurements"
Â  Â  headers = {"X-API-Key": API_KEY}
Â  Â  now = datetime.datetime.now(datetime.timezone.utc)
Â  Â  date_from = (now - datetime.timedelta(days=days)).isoformat().replace("+00:00", "Z")
Â  Â  params = {"limit": limit, "date_from": date_from}

Â  Â  try:
Â  Â  Â  Â  resp = requests.get(url, headers=headers, params=params)
Â  Â  Â  Â  resp.raise_for_status()
Â  Â  Â  Â  j = resp.json()
Â  Â  Â  Â  results = j.get("results", [])
Â  Â  except Exception as e:
Â  Â  Â  Â  print(f"âŒ æŠ“å– {param_name} æ•¸æ“šå¤±æ•—: {e}")
Â  Â  Â  Â  return pd.DataFrame()

Â  Â  rows = []
Â  Â  for r in results:
Â  Â  Â  Â  dt_str = _extract_datetime_from_measurement(r)
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  ts = pd.to_datetime(dt_str, utc=True)
Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  ts = pd.NaT
Â  Â  Â  Â  rows.append({"datetime": ts, param_name: r.get("value")})

Â  Â  df = pd.DataFrame(rows).dropna(subset=["datetime"])
Â  Â  if df.empty:
Â  Â  Â  Â  return pd.DataFrame()
Â  Â  df = df.sort_values("datetime", ascending=False).drop_duplicates(subset=["datetime"])
Â  Â  return df

def generate_fake_data(limit=10, params=POLLUTANT_TARGETS):
Â  Â  """ç”Ÿæˆæ‰€æœ‰ç›®æ¨™æ±¡æŸ“ç‰© (å« AQI) çš„æ¨¡æ“¬æ•¸æ“š"""
Â  Â  now = datetime.datetime.now(datetime.timezone.utc)
Â  Â  base_rows = []
Â  Â  for i in range(limit):
Â  Â  Â  Â  dt = now - datetime.timedelta(minutes=i*60)
Â  Â  Â  Â  row = {'datetime': dt}

Â  Â  Â  Â  for param in params:
Â  Â  Â  Â  Â  Â  if param in ["pm25", "pm10"]: value = round(random.uniform(10, 60), 1)
Â  Â  Â  Â  Â  Â  elif param == "o3": value = round(random.uniform(20, 100), 1)
Â  Â  Â  Â  Â  Â  elif param in ["no2", "so2"]: value = round(random.uniform(1, 40), 1)
Â  Â  Â  Â  Â  Â  elif param == 'co': value = round(random.uniform(0.1, 5), 1)
Â  Â  Â  Â  Â  Â  row[f'{param}_value'] = value

Â  Â  Â  Â  Â  # æ³¨æ„ï¼šæ­¤è™•æ¨¡æ“¬æ•¸æ“šä¸­æœªåŒ…å«å¤©æ°£ç‰¹å¾µï¼Œå¯¦éš›é‹è¡Œæ™‚æœƒå˜—è©¦æŠ“å– Meteostat æ•¸æ“š
Â  Â  Â  Â  row['temperature'] = round(random.uniform(15, 30), 1)
Â  Â  Â  Â  row['humidity'] = round(random.uniform(50, 95), 1)
Â  Â  Â  Â  row['pressure'] = round(random.uniform(1000, 1020), 1)

Â  Â  Â  Â  aqi_val = calculate_aqi(pd.Series(row), params)
Â  Â  Â  Â  row['aqi_value'] = aqi_val
Â  Â  Â  Â  base_rows.append(row)

Â  Â  df = pd.DataFrame(base_rows)
Â  Â  df['datetime'] = df['datetime'].dt.tz_localize('UTC')
Â  Â  return df

def get_all_target_data(station_id, target_params, days_to_fetch):
Â  Â  """ç²å–æ‰€æœ‰ç›®æ¨™æ±¡æŸ“ç‰©æ•¸æ“šä¸¦åˆä½µ"""
Â  Â  sensors = get_station_sensors(station_id)
Â  Â  sensor_map = {s.get("parameter", {}).get("name", "").lower(): s.get("id") for s in sensors}

Â  Â  all_dfs = []
Â  Â  found_params = []

Â  Â  for param in target_params:
Â  Â  Â  Â  sensor_id = sensor_map.get(param)
Â  Â  Â  Â  if sensor_id:
Â  Â  Â  Â  Â  Â  # ä½¿ç”¨ DAYS_TO_FETCH=2 å‘¼å«
Â  Â  Â  Â  Â  Â  df_param = fetch_sensor_data(sensor_id, param, days=days_to_fetch)
Â  Â  Â  Â  Â  Â  if not df_param.empty:
Â  Â  Â  Â  Â  Â  Â  Â  df_param.rename(columns={param: f'{param}_value'}, inplace=True)
Â  Â  Â  Â  Â  Â  Â  Â  all_dfs.append(df_param)
Â  Â  Â  Â  Â  Â  Â  Â  found_params.append(param)

Â  Â  if not all_dfs:
Â  Â  Â  Â  return pd.DataFrame(), []

Â  Â  merged_df = all_dfs[0]
Â  Â  for i in range(1, len(all_dfs)):
Â  Â  Â  Â  merged_df = pd.merge(merged_df, all_dfs[i], on='datetime', how='outer')

Â  Â  return merged_df, found_params


# =================================================================
# Meteostat å¤©æ°£çˆ¬èŸ²é¡ (æœªä¿®æ”¹)
# =================================================================
class WeatherCrawler:
Â  Â  """Meteostat å°æ™‚ç´šå¤©æ°£æ•¸æ“šçˆ¬èŸ²èˆ‡æ•´åˆ"""

Â  Â  def __init__(self, lat, lon):
Â  Â  Â  Â  self.point = Point(lat, lon)
Â  Â  Â  Â  self.weather_cols = {
Â  Â  Â  Â  Â  Â  'temp': 'temperature',
Â  Â  Â  Â  Â  Â  'rhum': 'humidity',
Â  Â  Â  Â  Â  Â  'pres': 'pressure',
Â  Â  Â  Â  }

Â  Â  def fetch_and_merge_weather(self, air_quality_df: pd.DataFrame):
Â  Â  Â  Â  """æ ¹æ“šç©ºæ°£å“è³ªæ•¸æ“šçš„æ™‚é–“ç¯„åœï¼Œå¾ Meteostat ç²å–å°æ™‚ç´šå¤©æ°£æ•¸æ“šä¸¦åˆä½µã€‚"""
Â  Â  Â  Â  if air_quality_df.empty:
Â  Â  Â  Â  Â  Â  return air_quality_df

Â  Â  Â  Â  if air_quality_df['datetime'].dt.tz is None:
Â  Â  Â  Â  Â  Â  Â air_quality_df['datetime'] = air_quality_df['datetime'].dt.tz_localize('UTC')

Â  Â  Â  Â  start_time_utc_aware = air_quality_df['datetime'].min()
Â  Â  Â  Â  end_time_utc_aware = air_quality_df['datetime'].max()

Â  Â  Â  Â  # Meteostat æœŸæœ›ç„¡æ™‚å€çš„ datetime ç‰©ä»¶
Â  Â  Â  Â  start_dt = start_time_utc_aware.tz_convert(None).to_pydatetime()
Â  Â  Â  Â  end_dt = end_time_utc_aware.tz_convert(None).to_pydatetime()

Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  data = Hourly(self.point, start_dt, end_dt)
Â  Â  Â  Â  Â  Â  weather_data = data.fetch()
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  print(f"âŒ æŠ“å– Meteostat æ•¸æ“šå¤±æ•—: {e}")
Â  Â  Â  Â  Â  Â  weather_data = pd.DataFrame()

Â  Â  Â  Â  if weather_data.empty:
Â  Â  Â  Â  Â  Â  # å¦‚æœæŠ“å–å¤±æ•—ï¼Œå‰‡å¡«å…… NaN
Â  Â  Â  Â  Â  Â  empty_weather = pd.DataFrame({'datetime': air_quality_df['datetime'].unique()})
Â  Â  Â  Â  Â  Â  for col in self.weather_cols.values():
Â  Â  Â  Â  Â  Â  Â  Â  empty_weather[col] = np.nan
Â  Â  Â  Â  Â  Â  return pd.merge(air_quality_df, empty_weather, on='datetime', how='left')

Â  Â  Â  Â  weather_data = weather_data.reset_index()
Â  Â  Â  Â  weather_data.rename(columns={'time': 'datetime'}, inplace=True)
Â  Â  Â  Â  weather_data = weather_data.rename(columns=self.weather_cols)
Â  Â  Â  Â  weather_data = weather_data[list(self.weather_cols.values()) + ['datetime']]
Â  Â  Â  Â  weather_data['datetime'] = weather_data['datetime'].dt.tz_localize('UTC')

Â  Â  Â  Â  merged_df = pd.merge(
Â  Â  Â  Â  Â  Â  air_quality_df,
Â  Â  Â  Â  Â  Â  weather_data,
Â  Â  Â  Â  Â  Â  on='datetime',
Â  Â  Â  Â  Â  Â  how='left'
Â  Â  Â  Â  )

Â  Â  Â  Â  weather_cols_list = list(self.weather_cols.values())
Â  Â  Â  Â  # ä½¿ç”¨ ffill/bfill è™•ç†ç¼ºå¤±å¤©æ°£æ•¸æ“š
Â  Â  Â  Â  merged_df[weather_cols_list] = merged_df[weather_cols_list].fillna(method='ffill').fillna(method='bfill')

Â  Â  Â  Â  return merged_df

Â  Â  def get_weather_feature_names(self):
Â  Â  Â  Â  return list(self.weather_cols.values())


# =================================================================
# é æ¸¬å‡½å¼ (æœªä¿®æ”¹)
# =================================================================

def predict_future_multi(models, last_data, feature_cols, pollutant_params, hours=24):
Â  Â  """é æ¸¬æœªä¾† N å°æ™‚çš„å¤šå€‹ç›®æ¨™æ±¡æŸ“ç‰© (éè¿´é æ¸¬) ä¸¦è¨ˆç®— AQI"""
Â  Â  predictions = []

Â  Â  last_datetime_aware = last_data['datetime'].iloc[0]
Â  Â  # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨ to_dict() å‰µå»ºä¸€å€‹å¯è®Šå­—å…¸å‰¯æœ¬ä½œç‚ºè¿­ä»£çš„åŸºç¤
Â  Â  current_data_dict = last_data[feature_cols].iloc[0].to_dict() 

Â  Â  weather_feature_names_base = ['temperature', 'humidity', 'pressure']
Â  Â  weather_feature_names = [col for col in weather_feature_names_base if col in feature_cols]
Â  Â  has_weather = bool(weather_feature_names)

Â  Â  for h in range(hours):
Â  Â  Â  Â  future_time = last_datetime_aware + timedelta(hours=h + 1)
Â  Â  Â  Â  pred_features = current_data_dict.copy()

Â  Â  Â  Â  # 1. æ›´æ–°æ™‚é–“ç‰¹å¾µ
Â  Â  Â  Â  pred_features['hour'] = future_time.hour
Â  Â  Â  Â  pred_features['day_of_week'] = future_time.dayofweek
Â  Â  Â  Â  pred_features['month'] = future_time.month
Â  Â  Â  Â  pred_features['day_of_year'] = future_time.timetuple().tm_yday # ä½¿ç”¨ day_of_year
Â  Â  Â  Â  pred_features['is_weekend'] = int(future_time.dayofweek in [5, 6])
Â  Â  Â  Â  pred_features['hour_sin'] = np.sin(2 * np.pi * future_time.hour / 24)
Â  Â  Â  Â  pred_features['hour_cos'] = np.cos(2 * np.pi * future_time.hour / 24)
Â  Â  Â  Â  pred_features['day_sin'] = np.sin(2 * np.pi * pred_features['day_of_year'] / 365)
Â  Â  Â  Â  pred_features['day_cos'] = np.cos(2 * np.pi * pred_features['day_of_year'] / 365)

Â  Â  Â  Â  # 2. æ¨¡æ“¬æœªä¾†å¤©æ°£è®ŠåŒ– (ä½¿ç”¨å‰ä¸€å°æ™‚çš„å¤©æ°£å€¼é€²è¡Œéš¨æ©Ÿæ“¾å‹•)
Â  Â  Â  Â  if has_weather:
Â  Â  Â  Â  Â  Â  np.random.seed(future_time.hour + future_time.day + 42)
Â  Â  Â  Â  Â  Â  for w_col in weather_feature_names:
Â  Â  Â  Â  Â  Â  Â  Â  base_value = current_data_dict.get(w_col)
Â  Â  Â  Â  Â  Â  Â  Â  if base_value is not None and not np.isnan(base_value):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # æ¨¡æ“¬è¼•å¾®éš¨æ©Ÿè®ŠåŒ–
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  new_weather_value = base_value + np.random.normal(0, 0.5) 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pred_features[w_col] = new_weather_value
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # å°‡æ–°çš„å¤©æ°£å€¼æ›´æ–°åˆ° current_data_dictï¼Œä»¥ä¾¿ä¸‹ä¸€å°æ™‚ä½¿ç”¨
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current_data_dict[w_col] = new_weather_value

Â  Â  Â  Â  current_prediction_row = {'datetime': future_time}
Â  Â  Â  Â  new_pollutant_values = {}

Â  Â  Â  Â  # 3. é æ¸¬æ‰€æœ‰æ±¡æŸ“ç‰©
Â  Â  Â  Â  for param in pollutant_params:
Â  Â  Â  Â  Â  Â  model = models[param]
Â  Â  Â  Â  Â  Â  # ç¢ºä¿è¼¸å…¥ç‰¹å¾µçš„é †åºèˆ‡æ¨¡å‹è¨“ç·´æ™‚ä¸€è‡´
Â  Â  Â  Â  Â  Â  pred_input = np.array([pred_features[col] for col in feature_cols]).reshape(1, -1)
Â  Â  Â  Â  Â  Â  pred = model.predict(pred_input)[0]
Â  Â  Â  Â  Â  Â  pred = max(0, pred) # æ¿ƒåº¦ä¸èƒ½å°æ–¼ 0

Â  Â  Â  Â  Â  Â  current_prediction_row[f'{param}_pred'] = pred
Â  Â  Â  Â  Â  Â  new_pollutant_values[param] = pred

Â  Â  Â  Â  # 4. è¨ˆç®—é æ¸¬çš„ AQI
Â  Â  Â  Â  predicted_aqi = calculate_aqi(pd.Series(current_prediction_row), pollutant_params)
Â  Â  Â  Â  current_prediction_row['aqi_pred'] = predicted_aqi
Â  Â  Â  Â  new_pollutant_values['aqi'] = predicted_aqi

Â  Â  Â  Â  predictions.append(current_prediction_row)

Â  Â  Â  Â  # 5. æ›´æ–°æ»¯å¾Œç‰¹å¾µ (éè¿´é æ¸¬çš„æ ¸å¿ƒ)
Â  Â  Â  Â  for param in pollutant_params + ['aqi']:
Â  Â  Â  Â  Â  Â  # å¾æœ€å¤§çš„ Lag é–‹å§‹æ›´æ–°ï¼Œé¿å…è¦†è“‹
Â  Â  Â  Â  Â  Â  for i in range(len(LAG_HOURS) - 1, 0, -1):
Â  Â  Â  Â  Â  Â  Â  Â  lag_current = LAG_HOURS[i]
Â  Â  Â  Â  Â  Â  Â  Â  lag_prev = LAG_HOURS[i-1]
Â  Â  Â  Â  Â  Â  Â  Â  lag_current_col = f'{param}_lag_{lag_current}h'
Â  Â  Â  Â  Â  Â  Â  Â  lag_prev_col = f'{param}_lag_{lag_prev}h'

Â  Â  Â  Â  Â  Â  Â  Â  if lag_current_col in current_data_dict and lag_prev_col in current_data_dict:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current_data_dict[lag_current_col] = current_data_dict[lag_prev_col]

Â  Â  Â  Â  Â  Â  # æ›´æ–° 1 å°æ™‚æ»¯å¾Œç‰¹å¾µç‚ºç•¶å‰é æ¸¬å€¼
Â  Â  Â  Â  Â  Â  if f'{param}_lag_1h' in current_data_dict and param in new_pollutant_values:
Â  Â  Â  Â  Â  Â  Â  Â  current_data_dict[f'{param}_lag_1h'] = new_pollutant_values[param]
Â  Â  Â  Â  
Â  Â  return pd.DataFrame(predictions)


# =================================================================
# æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•åˆå§‹åŒ– (åªåŸ·è¡Œä¸€æ¬¡)
# =================================================================

def initialize_app_data(lat: float, lon: float, days_to_fetch: int):
Â  Â  """
Â  Â  åŸ·è¡Œç©ºæ°£å“è³ªé æ¸¬çš„æ•´å€‹æµç¨‹ï¼Œä¸¦å°‡è¨“ç·´çµæœå„²å­˜åˆ°å…¨åŸŸè®Šæ•¸ä¸­ã€‚
Â  Â  æ­¤å‡½æ•¸åªåœ¨ Flask å•Ÿå‹•æ™‚åŸ·è¡Œä¸€æ¬¡ï¼Œé¿å… worker timeoutã€‚
Â  Â  """
Â  Â  global TRAINED_MODELS, LAST_OBSERVATION, FEATURE_COLUMNS, POLLUTANT_PARAMS
Â  Â  
Â  Â  weather = WeatherCrawler(lat, lon)
Â  Â  
Â  Â  try:
Â  Â  Â  Â  print("ğŸ”¥ [Init] é–‹å§‹åŸ·è¡Œ AQI é æ¸¬åˆå§‹åŒ–æµç¨‹...")
Â  Â  Â  Â  
Â  Â  Â  Â  # 1. æ•¸æ“šæ”¶é›† (ä½¿ç”¨ DAYS_TO_FETCH=2)
Â  Â  Â  Â  station = get_nearest_station(lat, lon, days=days_to_fetch) 

Â  Â  Â  Â  if not station:
Â  Â  Â  Â  Â  Â  print("ğŸš¨ [Init] æœªæ‰¾åˆ°æ´»èºæ¸¬ç«™ï¼Œä½¿ç”¨æ¨¡æ“¬æ•¸æ“šã€‚")
Â  Â  Â  Â  Â  Â  df = generate_fake_data(limit=days_to_fetch * 24, params=POLLUTANT_TARGETS)
Â  Â  Â  Â  Â  Â  found_target_params = POLLUTANT_TARGETS
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  print(f"âœ… [Init] æ‰¾åˆ°æ¸¬ç«™: {station['name']} ({station['id']})")
Â  Â  Â  Â  Â  Â  # ä½¿ç”¨ DAYS_TO_FETCH=2 å‘¼å«
Â  Â  Â  Â  Â  Â  df_raw, found_target_params = get_all_target_data(station["id"], POLLUTANT_TARGETS, days_to_fetch)

Â  Â  Â  Â  Â  Â  if df_raw.empty or len(df_raw) < MIN_DATA_THRESHOLD:
Â  Â  Â  Â  Â  Â  Â  Â  print("ğŸš¨ [Init] å¯¦éš›æ•¸æ“šé‡ä¸è¶³ï¼Œä½¿ç”¨æ¨¡æ“¬æ•¸æ“šã€‚")
Â  Â  Â  Â  Â  Â  Â  Â  df = generate_fake_data(limit=days_to_fetch * 24, params=POLLUTANT_TARGETS)
Â  Â  Â  Â  Â  Â  Â  Â  found_target_params = POLLUTANT_TARGETS
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  df = df_raw.copy()
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  # åˆä½µ Meteostat å¤©æ°£æ•¸æ“š
Â  Â  Â  Â  Â  Â  df = weather.fetch_and_merge_weather(df)

Â  Â  Â  Â  POLLUTANT_PARAMS = found_target_params
Â  Â  Â  Â  weather_feature_names = weather.get_weather_feature_names()
Â  Â  Â  Â  value_cols = [f'{p}_value' for p in POLLUTANT_PARAMS]
Â  Â  Â  Â  all_data_cols = value_cols + weather_feature_names

Â  Â  Â  Â  # é‡æ¡æ¨£åˆ°å°æ™‚
Â  Â  Â  Â  df.set_index('datetime', inplace=True)
Â  Â  Â  Â  df = df[value_cols + weather_feature_names].resample('H').mean()
Â  Â  Â  Â  df.reset_index(inplace=True)
Â  Â  Â  Â  df = df.dropna(how='all', subset=all_data_cols)
Â  Â  Â  Â  
Â  Â  Â  Â  # è¨ˆç®—æ­·å² AQI
Â  Â  Â  Â  df['aqi_value'] = df.apply(lambda row: calculate_aqi(row, POLLUTANT_PARAMS), axis=1)

Â  Â  Â  Â  # ç§»é™¤ä»»ä¸€æ±¡æŸ“ç‰©æˆ–å¤©æ°£æ•¸æ“šç‚º NaN çš„è¡Œ (ç¢ºä¿æ¨¡å‹è¼¸å…¥å®Œæ•´)
Â  Â  Â  Â  df = df.dropna(subset=all_data_cols + ['aqi_value']).reset_index(drop=True)
Â  Â  Â  Â  print(f"ğŸ“Š [Init] æœ€çµ‚ç”¨æ–¼è¨“ç·´çš„æ•¸æ“šé‡: {len(df)} å°æ™‚")


Â  Â  Â  Â  if len(df) <= max(LAG_HOURS):
Â  Â  Â  Â  Â  Â  raise ValueError(f"æœ€çµ‚æ•¸æ“šé‡ ({len(df)}) ä¸è¶³ {max(LAG_HOURS)}ï¼Œç„¡æ³•é€²è¡Œæ»¯å¾Œç‰¹å¾µå·¥ç¨‹å’Œè¨“ç·´ã€‚")


Â  Â  Â  Â  # 2. ç‰¹å¾µå·¥ç¨‹
Â  Â  Â  Â  df['hour'] = df['datetime'].dt.hour
Â  Â  Â  Â  df['day_of_week'] = df['datetime'].dt.dayofweek
Â  Â  Â  Â  df['month'] = df['datetime'].dt.month
Â  Â  Â  Â  df['day_of_year'] = df['datetime'].dt.dayofyear
Â  Â  Â  Â  df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
Â  Â  Â  Â  df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
Â  Â  Â  Â  df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
Â  Â  Â  Â  df['day_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)
Â  Â  Â  Â  df['day_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)

Â  Â  Â  Â  df = df.sort_values('datetime')
Â  Â  Â  Â  feature_base_cols = value_cols + ['aqi_value']

Â  Â  Â  Â  for col_name in feature_base_cols:
Â  Â  Â  Â  Â  Â  param = col_name.replace('_value', '')
Â  Â  Â  Â  Â  Â  # åƒ…æ·»åŠ æ»¯å¾Œç‰¹å¾µ (Lag features)
Â  Â  Â  Â  Â  Â  for lag in LAG_HOURS: 
Â  Â  Â  Â  Â  Â  Â  Â  df[f'{param}_lag_{lag}h'] = df[col_name].shift(lag)
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  # ç§»é™¤æ»¾å‹•å¹³å‡/æ¨™æº–å·®ç‰¹å¾µçš„å‰µå»º

Â  Â  Â  Â  df = df.dropna().reset_index(drop=True)

Â  Â  Â  Â  # å„²å­˜æœ€å¾Œä¸€ç­†æ•¸æ“šï¼Œç”¨æ–¼æœªä¾†é æ¸¬çš„èµ·é»
Â  Â  Â  Â  LAST_OBSERVATION = df.iloc[-1:].copy() 

Â  Â  Â  Â  base_time_features = ['hour', 'day_of_week', 'month', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos']
Â  Â  Â  Â  
Â  Â  Â  Â  air_quality_features = []
Â  Â  Â  Â  # åƒ…åŒ…å«æ»¯å¾Œç‰¹å¾µ
Â  Â  Â  Â  for param in POLLUTANT_PARAMS + ['aqi']:
Â  Â  Â  Â  Â  Â  for lag in LAG_HOURS:
Â  Â  Â  Â  Â  Â  Â  Â  air_quality_features.append(f'{param}_lag_{lag}h')


Â  Â  Â  Â  FEATURE_COLUMNS = weather_feature_names + base_time_features + air_quality_features
Â  Â  Â  Â  FEATURE_COLUMNS = [col for col in FEATURE_COLUMNS if col in df.columns]

Â  Â  Â  Â  # 3. æ•¸æ“šåˆ†å‰²èˆ‡æ¨¡å‹è¨“ç·´
Â  Â  Â  Â  split_idx = int(len(df) * 0.8)
Â  Â  Â  Â  X = df[FEATURE_COLUMNS]
Â  Â  Â  Â  Y = {param: df[f'{param}_value'] for param in POLLUTANT_PARAMS}
Â  Â  Â  Â  
Â  Â  Â  Â  X_train = X[:split_idx]
Â  Â  Â  Â  Y_train = {param: Y[param][:split_idx] for param in POLLUTANT_PARAMS}

Â  Â  Â  Â  # æ ¸å¿ƒè¨“ç·´æ­¥é©Ÿ
Â  Â  Â  Â  print(f"â³ [Init] é–‹å§‹è¨“ç·´ {len(POLLUTANT_PARAMS)} å€‹ XGBoost æ¨¡å‹ (N={N_ESTIMATORS}, Depth={MAX_DEPTH})...")
Â  Â  Â  Â  for param in POLLUTANT_PARAMS:
Â  Â  Â  Â  Â  Â  xgb_model = xgb.XGBRegressor(
Â  Â  Â  Â  Â  Â  Â  Â  n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH, learning_rate=0.08, random_state=42, n_jobs=-1 
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  # æ­¤è™•æ˜¯ä¸Šæ¬¡è¶…æ™‚çš„ä½ç½®ï¼Œç¾åœ¨æ•¸æ“šé‡å’Œæ¨¡å‹è¤‡é›œåº¦éƒ½å·²é™åˆ°æœ€ä½
Â  Â  Â  Â  Â  Â  xgb_model.fit(X_train, Y_train[param]) 
Â  Â  Â  Â  Â  Â  TRAINED_MODELS[param] = xgb_model
Â  Â  Â  Â  print("âœ… [Init] æ¨¡å‹è¨“ç·´å®Œæˆï¼Œæ‡‰ç”¨ç¨‹å¼æº–å‚™å°±ç·’ã€‚")

Â  Â  except Exception as e:
Â  Â  Â  Â  print(f"âŒ [Init] åˆå§‹åŒ–åŸ·è¡Œå¤±æ•—ï¼Œå°‡ä½¿ç”¨é è¨­ç©ºå€¼: {e}") 
Â  Â  Â  Â  TRAINED_MODELS = {} 
Â  Â  Â  Â  LAST_OBSERVATION = None
Â  Â  Â  Â  FEATURE_COLUMNS = []
Â  Â  Â  Â  POLLUTANT_PARAMS = []

# =================================================================
# Flask æ‡‰ç”¨ç¨‹å¼è¨­å®šèˆ‡å•Ÿå‹•
# =================================================================
app = Flask(__name__)

# æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•æ™‚ï¼Œç«‹å³åŸ·è¡Œåˆå§‹åŒ– (åœ¨ gunicorn å•Ÿå‹•æ™‚åŸ·è¡Œä¸€æ¬¡)
with app.app_context():
Â  Â  # é«˜é›„å¸‚ä¸­å¿ƒç¶“ç·¯åº¦
Â  Â  LAT, LON = 22.6273, 120.3014
Â  Â  # ä½¿ç”¨ DAYS_TO_FETCH=2 å‘¼å«
Â  Â  initialize_app_data(LAT, LON, DAYS_TO_FETCH) 

@app.route('/')
def index():
Â  Â  city_name = "é«˜é›„"
Â  Â  
Â  Â  # æª¢æŸ¥æ¨¡å‹æ˜¯å¦æˆåŠŸè¼‰å…¥
Â  Â  if TRAINED_MODELS and LAST_OBSERVATION is not None:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  # åƒ…åŸ·è¡Œå¿«é€Ÿçš„é æ¸¬é‚è¼¯ (predict_future_multi)
Â  Â  Â  Â  Â  Â  future_predictions = predict_future_multi(
Â  Â  Â  Â  Â  Â  Â  Â  TRAINED_MODELS,
Â  Â  Â  Â  Â  Â  Â  Â  LAST_OBSERVATION,
Â  Â  Â  Â  Â  Â  Â  Â  FEATURE_COLUMNS,
Â  Â  Â  Â  Â  Â  Â  Â  POLLUTANT_PARAMS,
Â  Â  Â  Â  Â  Â  Â  Â  hours=HOURS_TO_PREDICT
Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  # æ ¼å¼åŒ–çµæœ
Â  Â  Â  Â  Â  Â  future_predictions['datetime_local'] = future_predictions['datetime'].dt.tz_convert(LOCAL_TZ)
Â  Â  Â  Â  Â  Â  max_aqi = int(future_predictions['aqi_pred'].max())

Â  Â  Â  Â  Â  Â  aqi_predictions = [
Â  Â  Â  Â  Â  Â  Â  Â  {'time': item['datetime_local'].strftime('%Y-%m-%d %H:%M'), 'aqi': int(item['aqi_pred'])}
Â  Â  Â  Â  Â  Â  Â  Â  for item in future_predictions.to_dict(orient='records')
Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  max_aqi = "N/A"
Â  Â  Â  Â  Â  Â  aqi_predictions = []
Â  Â  Â  Â  Â  Â  print(f"âŒ [Request] é æ¸¬åŸ·è¡Œå¤±æ•—: {e}") 
Â  Â  else:
Â  Â  Â  Â  max_aqi = "N/A"
Â  Â  Â  Â  aqi_predictions = []
Â  Â  Â  Â  print("ğŸš¨ [Request] æ¨¡å‹æˆ–æ•¸æ“šå°šæœªåˆå§‹åŒ–ï¼Œç„¡æ³•é€²è¡Œé æ¸¬ã€‚")

Â  Â  return render_template('index.html', max_aqi=max_aqi, aqi_predictions=aqi_predictions, city_name=city_name)

if __name__ == '__main__':
Â  Â  # åœ¨æœ¬åœ°ç’°å¢ƒé‹è¡Œæ™‚ä½¿ç”¨
Â  Â  # æ³¨æ„ï¼šæœ¬åœ°é‹è¡Œå¯èƒ½ä»éœ€è¼ƒé•·æ™‚é–“ç­‰å¾…åˆå§‹åŒ–å®Œæˆ
Â  Â  app.run(debug=True)
