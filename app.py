# app.py - ä¾› Render éƒ¨ç½²ä½¿ç”¨ (å·²ç§»é™¤è€—æ™‚çš„è¨“ç·´é‚è¼¯)

# =================================================================
# å°å…¥æ‰€æœ‰å¿…è¦çš„åº«
# =================================================================
import requests
import pandas as pd
import datetime
import random
import re
import os
import warnings
import numpy as np
import xgboost as xgb
import json 
import io 
from datetime import timedelta, timezone
from flask import Flask, render_template

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# æ¨¡å‹èˆ‡å…ƒæ•¸æ“šè·¯å¾‘
MODELS_DIR = 'models'
META_PATH = os.path.join(MODELS_DIR, 'model_meta.json')

# =================================================================
# OpenAQ V3 å¸¸æ•¸è¨­å®š (æ–°å¢)
# =================================================================
# è«‹å‹™å¿…ä½¿ç”¨æ‚¨çš„å¯¦éš› API Keyï¼Œé€™è£¡ä½¿ç”¨ç¯„ä¾‹ Key
API_KEY = "98765df2082f04dc9449e305bc736e93624b66e250fa9dfabcca53b31fc11647"
headers = {"X-API-Key": API_KEY}
BASE = "https://api.openaq.org/v3"
LOCATION_ID = 2395624  # é«˜é›„å¸‚-å‰é‡‘

TARGET_PARAMS = ["co", "no2", "o3", "pm10", "pm25", "so2"]
PARAM_IDS = {"co": 8, "no2": 7, "o3": 10, "pm10": 1, "pm25": 2, "so2": 9}

# å°é½Šæ™‚é–“å…è¨±å®¹å¿ï¼ˆå…ˆç”¨ Â±5 åˆ†æ‰¾æ‰¹æ¬¡ï¼Œæ‰¾ä¸åˆ°å†æ”¾å¯¬åˆ° Â±60 åˆ†ï¼‰
TOL_MINUTES_PRIMARY = 5
TOL_MINUTES_FALLBACK = 60


# =================================================================
# å…¨åŸŸè®Šæ•¸ - æ”¹ç‚ºå¾æª”æ¡ˆè¼‰å…¥
# =================================================================
TRAINED_MODELS = {}
LAST_OBSERVATION = None # è¼‰å…¥è¨“ç·´æ™‚çš„æœ€å¾Œä¸€ç­†æ•¸æ“šï¼Œç”¨æ–¼æä¾› Lag/Weather ç‰¹å¾µ
FEATURE_COLUMNS = []
POLLUTANT_PARAMS = [] # å¯¦éš›æ‰¾åˆ°ä¸¦è¨“ç·´çš„æ¨¡å‹åƒæ•¸
HOURS_TO_PREDICT = 24

# =================================================================
# å¸¸æ•¸è¨­å®š (åƒ…ä¿ç•™èˆ‡é æ¸¬ç›¸é—œçš„å¸¸æ•¸)
# =================================================================
LOCAL_TZ = "Asia/Taipei"
LAG_HOURS = [1, 2, 3, 6, 12, 24] # é æ¸¬éè¿´éœ€è¦é€™äº›åƒæ•¸
ROLLING_WINDOWS = [6, 12, 24] # é æ¸¬éè¿´éœ€è¦é€™äº›åƒæ•¸
POLLUTANT_TARGETS = ["pm25", "pm10", "o3", "no2", "so2", "co"] # ç”¨æ–¼ AQI è¨ˆç®—

# ç°¡åŒ–çš„ AQI åˆ†ç´šè¡¨
AQI_BREAKPOINTS = {
Â  Â  "pm25": [(0.0, 12.0, 0, 50), (12.1, 35.4, 51, 100), (35.5, 55.4, 101, 150), (55.5, 150.4, 151, 200)],
Â  Â  "pm10": [(0, 54, 0, 50), (55, 154, 51, 100), (155, 254, 101, 150), (255, 354, 151, 200)],
Â  Â  "o3": [(0, 54, 0, 50), (55, 70, 51, 100), (71, 85, 101, 150), (86, 105, 151, 200)],
Â  Â  "co": [(0.0, 4.4, 0, 50), (4.5, 9.4, 51, 100), (9.5, 12.4, 101, 150), (12.5, 15.4, 151, 200)],
Â  Â  "no2": [(0, 100, 0, 50), (101, 360, 51, 100), (361, 649, 101, 150), (650, 1249, 151, 200)],
Â  Â  "so2": [(0, 35, 0, 50), (36, 75, 51, 100), (76, 185, 101, 150), (186, 304, 151, 200)],
}

# =================================================================
# è¼”åŠ©å‡½å¼: OpenAQ V3 æ•¸æ“šæŠ“å– (æ•´åˆè‡ªç¬¬ä¸€å€‹è…³æœ¬)
# =================================================================

def get_location_meta(location_id: int):
    """ç²å–ç«™é»çš„å…ƒæ•¸æ“šï¼ŒåŒ…å«æœ€å¾Œæ›´æ–°æ™‚é–“ã€‚"""
    r = requests.get(f"{BASE}/locations/{location_id}", headers=headers)
    r.raise_for_status()
    row = r.json()["results"][0]
    last_utc = pd.to_datetime(row["datetimeLast"]["utc"], errors="coerce", utc=True)
    last_local = row["datetimeLast"]["local"]
    return {
        "id": int(row["id"]),
        "name": row["name"],
        "last_utc": last_utc,
        "last_local": last_local,
    }


def get_location_latest_df(location_id: int) -> pd.DataFrame:
    """ç«™é»å„åƒæ•¸çš„ã€æœ€æ–°å€¼æ¸…å–®ã€â†’ æ­£è¦åŒ–æ™‚é–“æˆ ts_utc / ts_local"""
    r = requests.get(f"{BASE}/locations/{location_id}/latest", headers=headers, params={"limit": 1000})
    if r.status_code == 404:
        return pd.DataFrame()
    r.raise_for_status()
    results = r.json().get("results", [])
    if not results:
        return pd.DataFrame()

    df = pd.json_normalize(results)

    # åƒæ•¸åèˆ‡å–®ä½
    if "parameter.name" in df.columns:
        df["parameter"] = df["parameter.name"].str.lower()
    elif "parameter" in df.columns:
        df["parameter"] = df["parameter"].str.lower()
    else:
        df["parameter"] = None
    # è™•ç†å–®ä½æ¬„ä½ï¼Œä»¥é©æ‡‰ä¸åŒ API è¿”å›æ ¼å¼
    df["units"] = df.get("parameter.units") if "parameter.units" in df.columns else df.get("units")
    df["value"] = df["value"]

    # å–ä»£è¡¨è©²ç­†çš„UTCæ™‚é–“ï¼ˆä¾å„ªå…ˆåºï¼‰
    df["ts_utc"] = pd.NaT
    for col in ["datetime.utc", "period.datetimeTo.utc", "period.datetimeFrom.utc"]:
        if col in df.columns:
            ts = pd.to_datetime(df[col], errors="coerce", utc=True)
            df["ts_utc"] = df["ts_utc"].where(df["ts_utc"].notna(), ts)

    # åœ°æ–¹æ™‚é–“ï¼ˆè‹¥æœ‰ï¼‰
    local_col = None
    for c in ["datetime.local", "period.datetimeTo.local", "period.datetimeFrom.local"]:
        if c in df.columns:
            local_col = c
            break
    df["ts_local"] = df[local_col] if local_col else None

    return df[["parameter", "value", "units", "ts_utc", "ts_local"]]


def get_parameters_latest_df(location_id: int, target_params) -> pd.DataFrame:
    """ç”¨ /parameters/{pid}/latest?locationId= æ‹¿å„åƒæ•¸ã€æœ€æ–°å€¼ã€ä¸¦åˆä½µ"""
    rows = []
    for p in target_params:
        pid = PARAM_IDS[p]
        r = requests.get(
            f"{BASE}/parameters/{pid}/latest",
            headers=headers,
            params={"locationId": location_id, "limit": 50},
        )
        if r.status_code == 404:
            continue
        r.raise_for_status()
        res = r.json().get("results", [])
        if not res:
            continue
        df = pd.json_normalize(res)

        # åƒæ•¸åèˆ‡å–®ä½
        df["parameter"] = p
        df["units"] = df.get("parameter.units") if "parameter.units" in df.columns else df.get("units")
        df["value"] = df["value"]

        # æ™‚é–“æ¬„ä½
        df["ts_utc"] = pd.NaT
        for col in ["datetime.utc", "period.datetimeTo.utc", "period.datetimeFrom.utc"]:
            if col in df.columns:
                ts = pd.to_datetime(df[col], errors="coerce", utc=True)
                df["ts_utc"] = df["ts_utc"].where(df["ts_utc"].notna(), ts)

        local_col = None
        for c in ["datetime.local", "period.datetimeTo.local", "period.datetimeFrom.local"]:
            if c in df.columns:
                local_col = c
                break
        df["ts_local"] = df[local_col] if local_col else None

        rows.append(df[["parameter", "value", "units", "ts_utc", "ts_local"]])

    if not rows:
        return pd.DataFrame()
    return pd.concat(rows, ignore_index=True)


def pick_batch_near(df: pd.DataFrame, t_ref: pd.Timestamp, tol_minutes: int) -> pd.DataFrame:
    """å¾ DataFrame ä¸­é¸å‡ºæ™‚é–“æœ€æ¥è¿‘ t_ref ä¸”åœ¨å®¹å¿åº¦å…§çš„æ¯ç¨®åƒæ•¸çš„å–®ä¸€è§€æ¸¬å€¼ã€‚"""
    if df.empty or pd.isna(t_ref):
        return pd.DataFrame()

    df = df.copy()

    # ç¢ºä¿ ts_utc æ¬„ä½ä¸­çš„æ¯å€‹å€¼éƒ½æ˜¯å–®ä¸€æ™‚é–“æˆ³ï¼ˆè™•ç† list/ndarray çš„æƒ…æ³ï¼‰
    def _scalarize(v):
        if isinstance(v, (list, tuple, np.ndarray)):
            return v[0] if len(v) else None
        return v

    df["ts_utc"] = df["ts_utc"].map(_scalarize)
    df["ts_utc"] = pd.to_datetime(df["ts_utc"], errors="coerce", utc=True)

    # æ¥è‘—å°±èƒ½å®‰å…¨åšæ™‚é–“è·é›¢æ¯”è¼ƒ
    df["dt_diff"] = (df["ts_utc"] - t_ref).abs()

    tol = pd.Timedelta(minutes=tol_minutes)
    df = df[df["dt_diff"] <= tol]
    if df.empty:
        return df

    # æ’åºï¼šå„ªå…ˆåƒæ•¸ã€æ™‚é–“å·®æœ€å°ã€æ™‚é–“æˆ³æœ€æ–°
    df = df.sort_values(["parameter", "dt_diff", "ts_utc"], ascending=[True, True, False])
    # æ¯å€‹åƒæ•¸åªç•™ä¸€ç­†
    df = df.drop_duplicates(subset=["parameter"], keep="first")
    return df[["parameter", "value", "units", "ts_utc", "ts_local"]]


def fetch_latest_data_for_prediction(location_id: int, target_params: list, historical_data: pd.DataFrame) -> pd.DataFrame:
    """
    å¾ OpenAQ æŠ“å–æœ€æ–°ä¸€æ‰¹çš„ç›®æ¨™æ±¡æŸ“ç‰©è³‡æ–™ï¼Œä¸¦èˆ‡æ­·å²æ•¸æ“šåˆä½µä»¥å½¢æˆé æ¸¬è¼¸å…¥ã€‚
    """
    if historical_data.empty:
        print("âš ï¸ [Merge] ç¼ºå°‘è¨“ç·´æ™‚çš„æ­·å²æ•¸æ“š (LAST_OBSERVATION)ï¼Œç„¡æ³•æä¾› Lag/Weather ç‰¹å¾µã€‚")
        return pd.DataFrame()
    
    try:
        meta = get_location_meta(location_id)
        t_star_loc = meta["last_utc"]
        
        # 1. ç«™é»ã€æœ€æ–°å€¼æ¸…å–®ã€
        df_loc_latest = get_location_latest_df(location_id)
        if df_loc_latest.empty:
            print("âš ï¸ [Fetch] /locations/{id}/latest æ²’æœ‰ä»»ä½•è³‡æ–™ã€‚")
            return pd.DataFrame()

        # 2. ç¢ºå®šå°é½Šæ™‚é–“
        t_star_latest = df_loc_latest["ts_utc"].max()
        t_star = t_star_latest if pd.notna(t_star_latest) else t_star_loc
        if pd.isna(t_star):
             print("âš ï¸ [Fetch] ç„¡æ³•ç¢ºå®šæœ€æ–°æ‰¹æ¬¡æ™‚é–“ã€‚")
             return pd.DataFrame()

        # 3. æ‰¾æ¥è¿‘ t_star çš„ä¸€æ‰¹æ•¸æ“š (df_all)
        df_at_batch = pick_batch_near(df_loc_latest, t_star, TOL_MINUTES_PRIMARY)
        if df_at_batch.empty:
            df_at_batch = pick_batch_near(df_loc_latest, t_star, TOL_MINUTES_FALLBACK)

        have = set(df_at_batch["parameter"].str.lower().tolist()) if not df_at_batch.empty else set()
        missing = [p for p in TARGET_PARAMS if p not in have]
        df_param_batch = pd.DataFrame()

        if missing:
            df_param_latest = get_parameters_latest_df(location_id, missing)
            df_param_batch = pick_batch_near(df_param_latest, t_star, TOL_MINUTES_PRIMARY)
            if df_param_batch.empty:
                df_param_batch = pick_batch_near(df_param_latest, t_star, TOL_MINUTES_FALLBACK)
        
        frames = []
        if not df_at_batch.empty:
            frames.append(df_at_batch)
        if not df_param_batch.empty:
            frames.append(df_param_batch)
        
        if not frames:
            print(f"âš ï¸ [Fetch] åœ¨ {t_star} é™„è¿‘ï¼Œå…­é …æ±¡æŸ“ç‰©éƒ½æ²’æœ‰è³‡æ–™ã€‚")
            return pd.DataFrame()

        # æœ€çµ‚åˆä½µæ‰€æœ‰æœ€æ–°æ•¸æ“šï¼Œç¢ºä¿æ¯å€‹åƒæ•¸åªæœ‰ä¸€ç­†
        df_all = pd.concat(frames, ignore_index=True)
        df_all["dt_diff"] = (df_all["ts_utc"] - t_star).abs()
        df_all = df_all.sort_values(["parameter", "dt_diff", "ts_utc"], ascending=[True, True, False])
        df_all = df_all.drop_duplicates(subset=["parameter"], keep="first")
        
        # 4. å»ºç«‹å–®è¡Œæœ€æ–°è§€æ¸¬æ•¸æ“š (åªåŒ…å«æ™‚é–“å’Œæ±¡æŸ“ç‰©æ¿ƒåº¦)
        final_row = {'datetime': t_star}
        for _, row in df_all.iterrows():
            # ä½¿ç”¨ _value çµå°¾ä¾†åŒ¹é…æ¨¡å‹è¨“ç·´æ™‚çš„å‘½åè¦ç¯„
            final_row[f'{row["parameter"]}_value'] = row["value"] 

        current_obs_df = pd.DataFrame([final_row])

        # 5. åˆä½µæœ€æ–°è§€æ¸¬èˆ‡æ­·å²ç‰¹å¾µ (é‡é»æ­¥é©Ÿ)
        
        # è¤‡è£½æ­·å²ç‰¹å¾µ (åŒ…å«æ‰€æœ‰ lag/weather/seasonal features)
        # to_frame().T ç¢ºä¿å®ƒæ˜¯ä¸€å€‹å–®è¡Œ DataFrame
        input_df = historical_data.copy().iloc[0].to_frame().T
        
        # å¾æ­·å²æ•¸æ“šä¸­ç§»é™¤èˆŠçš„ datetime å’ŒèˆŠçš„æ±¡æŸ“ç‰©å€¼
        pollutant_value_cols = [f'{p}_value' for p in TARGET_PARAMS if f'{p}_value' in input_df.columns]
        input_df = input_df.drop(columns=['datetime'] + pollutant_value_cols, errors='ignore')
        
        # å°‡æœ€æ–°æŠ“åˆ°çš„æ•¸æ“šï¼ˆæ™‚é–“å’Œæ±¡æŸ“ç‰©å€¼ï¼‰åˆä½µåˆ° input_df ä¸­
        # concat å°‡æœƒæŠŠæœ€æ–°çš„æ•¸æ“šä½œç‚ºæ–°çš„ä¸€è¡Œ
        final_input_df = pd.concat([input_df, current_obs_df], axis=1).iloc[0].to_frame().T
        
        # ç¢ºä¿ final_input_df åªæœ‰éœ€è¦çš„æ¬„ä½ (feature_cols + 'datetime')
        required_cols = list(FEATURE_COLUMNS) + ['datetime']
        final_input_df = final_input_df.reindex(columns=required_cols, fill_value=np.nan)

        print(f"âœ… [Fetch] æˆåŠŸæŠ“å–æœ€æ–°æ‰¹æ¬¡è³‡æ–™ï¼Œæ™‚é–“: {t_star.strftime('%Y-%m-%d %H:%M:%S%Z')}")
        return final_input_df

    except requests.exceptions.RequestException as e:
        print(f"âŒ [Fetch] OpenAQ API è«‹æ±‚å¤±æ•—: {e}")
        return pd.DataFrame()
    except Exception as e:
        print(f"âŒ [Fetch] æŠ“å–æœ€æ–°è³‡æ–™æˆ–åˆä½µå¤±æ•—: {e}")
        return pd.DataFrame()


# =================================================================
# è¼”åŠ©å‡½å¼: AQI è¨ˆç®—
# =================================================================

def calculate_aqi_sub_index(param: str, concentration: float) -> float:
Â  Â  """è¨ˆç®—å–®ä¸€æ±¡æŸ“ç‰©æ¿ƒåº¦å°æ‡‰çš„ AQI å­æŒ‡æ•¸ (I)"""
Â  Â  if pd.isna(concentration) or concentration < 0:
Â  Â  Â  Â  return 0

Â  Â  breakpoints = AQI_BREAKPOINTS.get(param)
Â  Â  if not breakpoints:
Â  Â  Â  Â  return 0

Â  Â  for C_low, C_high, I_low, I_high in breakpoints:
Â  Â  Â  Â  if C_low <= concentration <= C_high:
Â  Â  Â  Â  Â  Â  if C_high == C_low:
Â  Â  Â  Â  Â  Â  Â  Â  return I_high
Â  Â  Â  Â  Â  Â  I = ((I_high - I_low) / (C_high - C_low)) * (concentration - C_low) + I_low
Â  Â  Â  Â  Â  Â  return np.round(I)

Â  Â  Â  Â  if concentration > breakpoints[-1][1]:
Â  Â  Â  Â  Â  Â  I_low, I_high = breakpoints[-1][2], breakpoints[-1][3]
Â  Â  Â  Â  Â  Â  C_low, C_high = breakpoints[-1][0], breakpoints[-1][1]
Â  Â  Â  Â  Â  Â  if C_high == C_low:
Â  Â  Â  Â  Â  Â  Â  Â  return I_high
Â  Â  Â  Â  Â  Â  I_rate = (I_high - I_low) / (C_high - C_low)
Â  Â  Â  Â  Â  Â  I = I_high + I_rate * (concentration - C_high)
Â  Â  Â  Â  Â  Â  return np.round(I)

Â  Â  return 0

def calculate_aqi(row: pd.Series, params: list) -> int:
Â  Â  """æ ¹æ“šå¤šå€‹æ±¡æŸ“ç‰©æ¿ƒåº¦è¨ˆç®—æœ€çµ‚ AQI (å–æœ€å¤§å­æŒ‡æ•¸)"""
Â  Â  sub_indices = []
Â  Â  for p in params:
Â  Â  Â  Â  # æª¢æŸ¥é æ¸¬å€¼å’Œè§€æ¸¬å€¼æ¬„ä½
Â  Â  Â  Â  col_pred = f'{p}_pred'
Â  Â  Â  Â  col_obs = f'{p}_value'
Â  Â  Â  Â  
Â  Â  Â  Â  # é æ¸¬å€¼å„ªå…ˆ
Â  Â  Â  Â  if col_pred in row and not pd.isna(row[col_pred]):
Â  Â  Â  Â  Â  Â  concentration = row[col_pred]
Â  Â  Â  Â  elif col_obs in row and not pd.isna(row[col_obs]):
Â  Â  Â  Â  Â  Â  concentration = row[col_obs]
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  sub_index = calculate_aqi_sub_index(p, concentration)
Â  Â  Â  Â  sub_indices.append(sub_index)

Â  Â  if not sub_indices:
Â  Â  Â  Â  return np.nan

Â  Â  return int(np.max(sub_indices))

# =================================================================
# é æ¸¬å‡½å¼
# =================================================================

def predict_future_multi(models, last_data, feature_cols, pollutant_params, hours=24):
Â  Â  """é æ¸¬æœªä¾† N å°æ™‚çš„å¤šå€‹ç›®æ¨™æ±¡æŸ“ç‰© (éè¿´é æ¸¬) ä¸¦è¨ˆç®— AQI"""
Â  Â  predictions = []

Â  Â  # last_data ç¾åœ¨æ˜¯å–®è¡Œ DataFrameï¼Œéœ€è¦ç¢ºä¿å…¶æ™‚é–“æ ¼å¼å’Œæ™‚å€è¨­å®šæ­£ç¢º
Â  Â  last_data['datetime'] = pd.to_datetime(last_data['datetime'])

Â  Â  # ä¿®æ­£æ™‚å€å•é¡Œ: ç¢ºä¿æ—¥æœŸæ™‚é–“ç‚º UTC-awareã€‚
Â  Â  if last_data['datetime'].dt.tz is not None:
Â  Â  Â  Â  last_data['datetime'] = last_data['datetime'].dt.tz_convert('UTC')
Â  Â  else:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  last_data['datetime'] = last_data['datetime'].dt.tz_localize('UTC')
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  if "tz-aware" in str(e):
Â  Â  Â  Â  Â  Â  Â  Â  print("âš ï¸ [TZ Fix] .dt.tz æª¢æŸ¥å¤±æ•ˆï¼Œå¯¦éš›ç‚º tz-awareï¼Œä½¿ç”¨ tz_convert ä¿®æ­£ã€‚")
Â  Â  Â  Â  Â  Â  Â  Â  last_data['datetime'] = last_data['datetime'].dt.tz_convert('UTC')
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  raise e


Â  Â  last_datetime_aware = last_data['datetime'].iloc[0]
Â  Â  # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨ to_dict() å‰µå»ºä¸€å€‹å¯è®Šå­—å…¸å‰¯æœ¬ä½œç‚ºè¿­ä»£çš„åŸºç¤
Â  Â  current_data_dict = last_data[feature_cols].iloc[0].to_dict()

Â  Â  weather_feature_names_base = ['temperature', 'humidity', 'pressure']
Â  Â  weather_feature_names = [col for col in weather_feature_names_base if col in feature_cols]
Â  Â  has_weather = bool(weather_feature_names)

Â  Â  for h in range(hours):
Â  Â  Â  Â  future_time = last_datetime_aware + timedelta(hours=h + 1)
Â  Â  Â  Â  pred_features = current_data_dict.copy()

Â  Â  Â  Â  # 1. æ›´æ–°æ™‚é–“ç‰¹å¾µ
Â  Â  Â  Â  pred_features['hour'] = future_time.hour
Â  Â  Â  Â  pred_features['day_of_week'] = future_time.dayofweek
Â  Â  Â  Â  pred_features['month'] = future_time.month
Â  Â  Â  Â  pred_features['day_of_year'] = future_time.timetuple().tm_yday # ä½¿ç”¨ day_of_year
Â  Â  Â  Â  pred_features['is_weekend'] = int(future_time.dayofweek in [5, 6])
Â  Â  Â  Â  pred_features['hour_sin'] = np.sin(2 * np.pi * future_time.hour / 24)
Â  Â  Â  Â  pred_features['hour_cos'] = np.cos(2 * np.pi * future_time.hour / 24)
Â  Â  Â  Â  pred_features['day_sin'] = np.sin(2 * np.pi * pred_features['day_of_year'] / 365)
Â  Â  Â  Â  pred_features['day_cos'] = np.cos(2 * np.pi * pred_features['day_of_year'] / 365)

Â  Â  Â  Â  # 2. æ¨¡æ“¬æœªä¾†å¤©æ°£è®ŠåŒ– (ä½¿ç”¨å‰ä¸€å°æ™‚çš„å¤©æ°£å€¼é€²è¡Œéš¨æ©Ÿæ“¾å‹•)
Â  Â  Â  Â  if has_weather:
Â  Â  Â  Â  Â  Â  # ç¢ºä¿ç¨®å­èˆ‡æ™‚é–“ç›¸é—œï¼Œè®“æ¯æ¬¡é‹è¡Œçµæœä¸€è‡´ï¼Œä½†æ¯å€‹å°æ™‚ä¸åŒ
Â  Â  Â  Â  Â  Â  np.random.seed(future_time.year + future_time.month + future_time.day + future_time.hour + 42)
Â  Â  Â  Â  Â  Â  for w_col in weather_feature_names:
Â  Â  Â  Â  Â  Â  Â  Â  base_value = current_data_dict.get(w_col)
Â  Â  Â  Â  Â  Â  Â  Â  if base_value is not None and not np.isnan(base_value):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # æ¨¡æ“¬è¼•å¾®éš¨æ©Ÿè®ŠåŒ– (ä½¿ç”¨å‰ä¸€å°æ™‚çš„å¤©æ°£å€¼é€²è¡Œéè¿´)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  new_weather_value = base_value + np.random.normal(0, 0.5)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pred_features[w_col] = new_weather_value
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # å°‡æ–°çš„å¤©æ°£å€¼æ›´æ–°åˆ° current_data_dictï¼Œä»¥ä¾¿ä¸‹ä¸€å°æ™‚ä½¿ç”¨
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current_data_dict[w_col] = new_weather_value

Â  Â  Â  Â  current_prediction_row = {'datetime': future_time}
Â  Â  Â  Â  new_pollutant_values = {}

Â  Â  Â  Â  # 3. é æ¸¬æ‰€æœ‰æ±¡æŸ“ç‰©
Â  Â  Â  Â  for param in pollutant_params:
Â  Â  Â  Â  Â  Â  model = models[param]
Â  Â  Â  Â  Â  Â  # ç¢ºä¿è¼¸å…¥ç‰¹å¾µçš„é †åºèˆ‡æ¨¡å‹è¨“ç·´æ™‚ä¸€è‡´
Â  Â  Â  Â  Â  Â  pred_input_data = {col: pred_features.get(col, 0) for col in feature_cols}
Â  Â  Â  Â  Â  Â  pred_input = np.array([pred_input_data[col] for col in feature_cols]).reshape(1, -1)
Â  Â  Â  Â  Â  Â  pred = model.predict(pred_input)[0]
Â  Â  Â  Â  Â  Â  pred = max(0, pred) # æ¿ƒåº¦ä¸èƒ½å°æ–¼ 0

Â  Â  Â  Â  Â  Â  current_prediction_row[f'{param}_pred'] = pred
Â  Â  Â  Â  Â  Â  new_pollutant_values[param] = pred

Â  Â  Â  Â  # 4. è¨ˆç®—é æ¸¬çš„ AQI
Â  Â  Â  Â  predicted_aqi = calculate_aqi(pd.Series(current_prediction_row), pollutant_params)
Â  Â  Â  Â  current_prediction_row['aqi_pred'] = predicted_aqi
Â  Â  Â  Â  new_pollutant_values['aqi'] = predicted_aqi

Â  Â  Â  Â  predictions.append(current_prediction_row)

Â  Â  Â  Â  # 5. æ›´æ–°æ»¯å¾Œç‰¹å¾µ (éè¿´æ›´æ–°)
Â  Â  Â  Â  for param in pollutant_params + ['aqi']:
Â  Â  Â  Â  Â  Â  # å¾æœ€å¤§çš„ Lag é–‹å§‹æ›´æ–°ï¼Œé¿å…è¦†è“‹
Â  Â  Â  Â  Â  Â  for i in range(len(LAG_HOURS) - 1, 0, -1):
Â  Â  Â  Â  Â  Â  Â  Â  lag_current = LAG_HOURS[i]
Â  Â  Â  Â  Â  Â  Â  Â  lag_prev = LAG_HOURS[i-1]
Â  Â  Â  Â  Â  Â  Â  Â  lag_current_col = f'{param}_lag_{lag_current}h'
Â  Â  Â  Â  Â  Â  Â  Â  lag_prev_col = f'{param}_lag_{lag_prev}h'

Â  Â  Â  Â  Â  Â  Â  Â  if lag_current_col in current_data_dict and lag_prev_col in current_data_dict:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current_data_dict[lag_current_col] = current_data_dict[lag_prev_col]

Â  Â  Â  Â  Â  Â  # æ›´æ–° 1 å°æ™‚æ»¯å¾Œç‰¹å¾µç‚ºç•¶å‰é æ¸¬å€¼
Â  Â  Â  Â  Â  Â  if f'{param}_lag_1h' in current_data_dict and param in new_pollutant_values:
Â  Â  Â  Â  Â  Â  Â  Â  current_data_dict[f'{param}_lag_1h'] = new_pollutant_values[param]

Â  Â  Â  Â  # 6. æ»¾å‹•å¹³å‡/æ¨™æº–å·®ç‰¹å¾µç„¡æ³•åœ¨éè¿´ä¸­æº–ç¢ºæ›´æ–°ï¼Œé€™è£¡ä¿æŒçœç•¥


Â  Â  return pd.DataFrame(predictions)


# =================================================================
# æ¨¡å‹è¼‰å…¥é‚è¼¯
# =================================================================

def load_models_and_metadata():
Â  Â  global TRAINED_MODELS, LAST_OBSERVATION, FEATURE_COLUMNS, POLLUTANT_PARAMS

Â  Â  if not os.path.exists(META_PATH):
Â  Â  Â  Â  print("ğŸš¨ [Load] æ‰¾ä¸åˆ°æ¨¡å‹å…ƒæ•¸æ“šæª”æ¡ˆ (model_meta.json)ï¼Œç„¡æ³•è¼‰å…¥æ¨¡å‹ã€‚")
Â  Â  Â  Â  return

Â  Â  try:
Â  Â  Â  Â  # 1. è¼‰å…¥å…ƒæ•¸æ“š
Â  Â  Â  Â  with open(META_PATH, 'r', encoding='utf-8') as f:
Â  Â  Â  Â  Â  Â  metadata = json.load(f)

Â  Â  Â  Â  POLLUTANT_PARAMS = metadata.get('pollutant_params', [])
Â  Â  Â  Â  FEATURE_COLUMNS = metadata.get('feature_columns', [])

Â  Â  Â  Â  # å°‡æœ€å¾Œä¸€ç­†æ•¸æ“šçš„ JSON è½‰æ›å› DataFrame
Â  Â  Â  Â  if 'last_observation_json' in metadata:
Â  Â  Â  Â  Â  Â  # ä½¿ç”¨ StringIO æ¨¡æ“¬æª”æ¡ˆè®€å–ï¼Œç¢ºä¿æ ¼å¼æ­£ç¢º
Â  Â  Â  Â  Â  Â  LAST_OBSERVATION = pd.read_json(io.StringIO(metadata['last_observation_json']), orient='records')

Â  Â  Â  Â  # 2. è¼‰å…¥ XGBoost æ¨¡å‹
Â  Â  Â  Â  TRAINED_MODELS = {}
Â  Â  Â  Â  params_to_check = list(POLLUTANT_PARAMS)

Â  Â  Â  Â  for param in params_to_check:
Â  Â  Â  Â  Â  Â  model_path = os.path.join(MODELS_DIR, f'{param}_model.json')
Â  Â  Â  Â  Â  Â  if os.path.exists(model_path):
Â  Â  Â  Â  Â  Â  Â  Â  model = xgb.XGBRegressor()
Â  Â  Â  Â  Â  Â  Â  Â  model.load_model(model_path)
Â  Â  Â  Â  Â  Â  Â  Â  TRAINED_MODELS[param] = model
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  print(f"âŒ [Load] æ‰¾ä¸åˆ° {param} çš„æ¨¡å‹æª”æ¡ˆ: {model_path}")

Â  Â  Â  Â  # æœ€çµ‚æ›´æ–° POLLUTANT_PARAMSï¼Œåªä¿ç•™æˆåŠŸè¼‰å…¥æ¨¡å‹çš„
Â  Â  Â  Â  POLLUTANT_PARAMS = list(TRAINED_MODELS.keys())

Â  Â  Â  Â  if TRAINED_MODELS:
Â  Â  Â  Â  Â  Â  print(f"âœ… [Load] æˆåŠŸè¼‰å…¥ {len(TRAINED_MODELS)} å€‹æ¨¡å‹ã€‚")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  print("ğŸš¨ [Load] æœªè¼‰å…¥ä»»ä½•æ¨¡å‹ã€‚")


Â  Â  except Exception as e:
Â  Â  Â  Â  print(f"âŒ [Load] æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
Â  Â  Â  Â  TRAINED_MODELS = {}
Â  Â  Â  Â  LAST_OBSERVATION = None
Â  Â  Â  Â  FEATURE_COLUMNS = []
Â  Â  Â  Â  POLLUTANT_PARAMS = []

# =================================================================
# Flask æ‡‰ç”¨ç¨‹å¼è¨­å®šèˆ‡å•Ÿå‹•
# =================================================================
app = Flask(__name__)

# æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•æ™‚ï¼Œç«‹å³åŸ·è¡Œæ¨¡å‹è¼‰å…¥ (å¿«é€Ÿ)
with app.app_context():
Â  Â  load_models_and_metadata()

@app.route('/')
def index():
Â  Â  global LAST_OBSERVATION # å…è¨±è®€å–å…¨åŸŸè®Šæ•¸
Â  Â  city_name = "é«˜é›„"
Â  Â  
Â  Â  # æª¢æŸ¥æ¨¡å‹æ˜¯å¦æˆåŠŸè¼‰å…¥
Â  Â  if TRAINED_MODELS and LAST_OBSERVATION is not None and not LAST_OBSERVATION.empty:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  # 1. ğŸš¨ å³æ™‚æŠ“å–æœ€æ–°è§€æ¸¬æ•¸æ“šä¸¦èˆ‡æ­·å²æ•¸æ“šåˆä½µ ğŸš¨
Â  Â  Â  Â  Â  Â  # ç²å–å–®è¡Œä¸”æ›´æ–°äº†æœ€æ–°æ±¡æŸ“ç‰©å€¼å’Œæ™‚é–“æˆ³çš„ DataFrame
Â  Â  Â  Â  Â  Â  final_input_data = fetch_latest_data_for_prediction(
Â  Â  Â  Â  Â  Â  Â  Â  LOCATION_ID,
Â  Â  Â  Â  Â  Â  Â  Â  TARGET_PARAMS,
Â  Â  Â  Â  Â  Â  Â  Â  LAST_OBSERVATION.copy() # å‚³éå‰¯æœ¬
Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  if final_input_data.empty or 'datetime' not in final_input_data.columns:
Â  Â  Â  Â  Â  Â  Â  Â  max_aqi = "N/A"
Â  Â  Â  Â  Â  Â  Â  Â  aqi_predictions = []
Â  Â  Â  Â  Â  Â  Â  Â  print("ğŸš¨ [Request] ç„¡æ³•å–å¾—æœ€æ–°æ•¸æ“šï¼Œæˆ–åˆä½µæ•¸æ“šæ ¼å¼éŒ¯èª¤ã€‚")
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  # 2. åŸ·è¡Œé æ¸¬
Â  Â  Â  Â  Â  Â  Â  Â  future_predictions = predict_future_multi(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  TRAINED_MODELS,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  final_input_data, # ä½¿ç”¨å³æ™‚ä¸”åˆä½µå¾Œçš„æ•¸æ“š
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  FEATURE_COLUMNS,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  POLLUTANT_PARAMS,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hours=HOURS_TO_PREDICT
Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  # æ ¼å¼åŒ–çµæœ
Â  Â  Â  Â  Â  Â  Â  Â  future_predictions['datetime_local'] = future_predictions['datetime'].dt.tz_convert(LOCAL_TZ)

Â  Â  Â  Â  Â  Â  Â  Â  # ç¢ºä¿ aqi_pred æ˜¯æ•¸å­—å†å– max
Â  Â  Â  Â  Â  Â  Â  Â  if not future_predictions.empty and future_predictions['aqi_pred'].dtype in [np.int64, np.float64]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  max_aqi = int(future_predictions['aqi_pred'].max())
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  max_aqi = "N/A"

Â  Â  Â  Â  Â  Â  Â  Â  aqi_predictions = [
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {'time': item['datetime_local'].strftime('%Y-%m-%d %H:%M'), 'aqi': int(item['aqi_pred'])}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for item in future_predictions.to_dict(orient='records')
Â  Â  Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  max_aqi = "N/A"
Â  Â  Â  Â  Â  Â  aqi_predictions = []
Â  Â  Â  Â  Â  Â  print(f"âŒ [Request] é æ¸¬åŸ·è¡Œå¤±æ•—: {e}")

Â  Â  else:
Â  Â  Â  Â  max_aqi = "N/A"
Â  Â  Â  Â  aqi_predictions = []
Â  Â  Â  Â  print("ğŸš¨ [Request] æ¨¡å‹æˆ–æ•¸æ“šå°šæœªåˆå§‹åŒ–ï¼Œç„¡æ³•é€²è¡Œé æ¸¬ã€‚")

Â  Â  return render_template('index.html', max_aqi=max_aqi, aqi_predictions=aqi_predictions, city_name=city_name)

if __name__ == '__main__':
Â  Â  # åœ¨æœ¬åœ°ç’°å¢ƒé‹è¡Œæ™‚ä½¿ç”¨
Â  Â  app.run(debug=True)
