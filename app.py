# app.py - ä¾› Render éƒ¨ç½²ä½¿ç”¨ (å·²ç§»é™¤è€—æ™‚çš„è¨“ç·´é‚è¼¯)

# =================================================================
# å°å…¥æ‰€æœ‰å¿…è¦çš„åº« (æ–°å¢ requests, numpy, pandas, json, xgboost, datetime çš„å°å…¥)
# =================================================================
import requests
import pandas as pd
import datetime
import random
import re
import os
import warnings
import numpy as np
import xgboost as xgb
import json
from datetime import timedelta, timezone
from flask import Flask, render_template

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# æ¨¡å‹èˆ‡å…ƒæ•¸æ“šè·¯å¾‘
MODELS_DIR = 'models'
META_PATH = os.path.join(MODELS_DIR, 'model_meta.json')

# =================================================================
# OpenAQ API ç›¸é—œå¸¸æ•¸ (å¾æ‚¨çš„ç¬¬ä¸€å€‹è…³æœ¬è¤‡è£½)
# =================================================================
# âš ï¸ è«‹æ›¿æ›æˆæ‚¨è‡ªå·±çš„ API Key
API_KEY = "98765df2082f04dc9449e305bc736e93624b66e250fa9dfabcca53b31fc11647" 
HEADERS = {"X-API-Key": API_KEY}
BASE = "https://api.openaq.org/v3"

LOCATION_ID = 2395624  # é«˜é›„å¸‚-å‰é‡‘ (è«‹æ›¿æ›ç‚ºæ‚¨éœ€è¦çš„ç«™é» ID)
TARGET_PARAMS = ["co", "no2", "o3", "pm10", "pm25", "so2"]
PARAM_IDS = {"co": 8, "no2": 7, "o3": 10, "pm10": 1, "pm25": 2, "so2": 9}

TOL_MINUTES_PRIMARY = 5
TOL_MINUTES_FALLBACK = 60

# =================================================================
# å…¨åŸŸè®Šæ•¸ - æ”¹ç‚ºå¾æª”æ¡ˆè¼‰å…¥
# =================================================================
TRAINED_MODELS = {} 
# âš ï¸ LAST_OBSERVATION ä¸å†å¾æª”æ¡ˆè¼‰å…¥ï¼Œæ”¹ç‚ºå³æ™‚æŠ“å–ï¼Œä½†åœ¨è¼‰å…¥æ™‚ä»è®€å–ä»¥å‚™æ¨¡å‹éœ€è¦
LAST_OBSERVATION = None 
FEATURE_COLUMNS = []
POLLUTANT_PARAMS = [] # å¯¦éš›æ‰¾åˆ°ä¸¦è¨“ç·´çš„æ¨¡å‹åƒæ•¸
HOURS_TO_PREDICT = 24

# =================================================================
# å¸¸æ•¸è¨­å®š
# =================================================================
LOCAL_TZ = "Asia/Taipei"
LAG_HOURS = [1, 2, 3, 6, 12, 24]
ROLLING_WINDOWS = [6, 12, 24]
POLLUTANT_TARGETS = ["pm25", "pm10", "o3", "no2", "so2", "co"]
# ... (AQI_BREAKPOINTS ä¿æŒä¸è®Š) ...
AQI_BREAKPOINTS = {
    "pm25": [(0.0, 12.0, 0, 50), (12.1, 35.4, 51, 100), (35.5, 55.4, 101, 150), (55.5, 150.4, 151, 200)],
    "pm10": [(0, 54, 0, 50), (55, 154, 51, 100), (155, 254, 101, 150), (255, 354, 151, 200)],
    "o3": [(0, 54, 0, 50), (55, 70, 51, 100), (71, 85, 101, 150), (86, 105, 151, 200)],
    "co": [(0.0, 4.4, 0, 50), (4.5, 9.4, 51, 100), (9.5, 12.4, 101, 150), (12.5, 15.4, 151, 200)],
    "no2": [(0, 100, 0, 50), (101, 360, 51, 100), (361, 649, 101, 150), (650, 1249, 151, 200)],
    "so2": [(0, 35, 0, 50), (36, 75, 51, 100), (76, 185, 101, 150), (186, 304, 151, 200)],
}


# =================================================================
# OpenAQ è³‡æ–™æŠ“å–å‡½å¼ (å¾æ‚¨çš„ç¬¬ä¸€å€‹è…³æœ¬è¤‡è£½éä¾†)
# =================================================================

def get_location_meta(location_id: int):
    """å–å¾—ç«™é»æœ€å¾Œæ›´æ–°æ™‚é–“"""
    try:
        r = requests.get(f"{BASE}/locations/{location_id}", headers=HEADERS, timeout=10)
        r.raise_for_status()
        row = r.json()["results"][0]
        last_utc = pd.to_datetime(row["datetimeLast"]["utc"], errors="coerce", utc=True)
        last_local = row["datetimeLast"]["local"]
        return {
            "id": int(row["id"]),
            "name": row["name"],
            "last_utc": last_utc,
            "last_local": last_local,
        }
    except Exception as e:
        print(f"âŒ [Fetch] get_location_meta å¤±æ•—: {e}")
        return None


def get_location_latest_df(location_id: int) -> pd.DataFrame:
    """ç«™é»å„åƒæ•¸çš„ã€æœ€æ–°å€¼æ¸…å–®ã€â†’ æ­£è¦åŒ–æ™‚é–“æˆ ts_utc / ts_local"""
    try:
        r = requests.get(f"{BASE}/locations/{location_id}/latest", headers=HEADERS, params={"limit": 1000}, timeout=10)
        if r.status_code == 404:
            return pd.DataFrame()
        r.raise_for_status()
        results = r.json().get("results", [])
        if not results:
            return pd.DataFrame()

        df = pd.json_normalize(results)

        # åƒæ•¸åèˆ‡å–®ä½
        df["parameter"] = df["parameter.name"].str.lower() if "parameter.name" in df.columns else df.get("parameter", df.get("name"))
        df["units"] = df["parameter.units"] if "parameter.units" in df.columns else df.get("units")
        df["value"] = df["value"]

        # å–ä»£è¡¨è©²ç­†çš„UTCæ™‚é–“
        df["ts_utc"] = pd.NaT
        for col in ["datetime.utc", "period.datetimeTo.utc", "period.datetimeFrom.utc"]:
            if col in df.columns:
                ts = pd.to_datetime(df[col], errors="coerce", utc=True)
                df["ts_utc"] = df["ts_utc"].where(df["ts_utc"].notna(), ts)

        # åœ°æ–¹æ™‚é–“
        local_col = None
        for c in ["datetime.local", "period.datetimeTo.local", "period.datetimeFrom.local"]:
            if c in df.columns:
                local_col = c
                break
        df["ts_local"] = df[local_col] if local_col in df.columns else None

        return df[["parameter", "value", "units", "ts_utc", "ts_local"]]
    except Exception as e:
        print(f"âŒ [Fetch] get_location_latest_df å¤±æ•—: {e}")
        return pd.DataFrame()


def get_parameters_latest_df(location_id: int, target_params) -> pd.DataFrame:
    """ç”¨ /parameters/{pid}/latest?locationId= æ‹¿å„åƒæ•¸ã€æœ€æ–°å€¼ã€ä¸¦åˆä½µ"""
    rows = []
    try:
        for p in target_params:
            pid = PARAM_IDS.get(p)
            if not pid: continue
            r = requests.get(
                f"{BASE}/parameters/{pid}/latest",
                headers=HEADERS,
                params={"locationId": location_id, "limit": 50},
                timeout=10
            )
            if r.status_code == 404:
                continue
            r.raise_for_status()
            res = r.json().get("results", [])
            if not res:
                continue
            df = pd.json_normalize(res)

            # åƒæ•¸åèˆ‡å–®ä½
            df["parameter"] = p
            df["units"] = df["parameter.units"] if "parameter.units" in df.columns else df.get("units")
            df["value"] = df["value"]

            # æ™‚é–“æ¬„ä½
            df["ts_utc"] = pd.NaT
            for col in ["datetime.utc", "period.datetimeTo.utc", "period.datetimeFrom.utc"]:
                if col in df.columns:
                    ts = pd.to_datetime(df[col], errors="coerce", utc=True)
                    df["ts_utc"] = df["ts_utc"].where(df["ts_utc"].notna(), ts)

            local_col = None
            for c in ["datetime.local", "period.datetimeTo.local", "period.datetimeFrom.local"]:
                if c in df.columns:
                    local_col = c
                    break
            df["ts_local"] = df[local_col] if local_col in df.columns else None

            rows.append(df[["parameter", "value", "units", "ts_utc", "ts_local"]])

    except Exception as e:
        print(f"âŒ [Fetch] get_parameters_latest_df å¤±æ•—: {e}")

    if not rows:
        return pd.DataFrame()
    return pd.concat(rows, ignore_index=True)


def pick_batch_near(df: pd.DataFrame, t_ref: pd.Timestamp, tol_minutes: int) -> pd.DataFrame:
    """å¾ DataFrame ä¸­æŒ‘é¸æœ€æ¥è¿‘ t_ref ä¸”æ™‚é–“å·®ç•°åœ¨ tol_minutes å…§çš„è³‡æ–™æ‰¹æ¬¡"""
    if df.empty or pd.isna(t_ref):
        return pd.DataFrame()

    df = df.copy()

    # â˜… ç¢ºä¿ ts_utc æ˜¯å–®ä¸€å€¼ä¸”ç‚º NaT-aware
    def _scalarize(v):
        if isinstance(v, (list, tuple, np.ndarray)):
            return v[0] if len(v) else None
        return v

    df["ts_utc"] = df["ts_utc"].map(_scalarize)
    df["ts_utc"] = pd.to_datetime(df["ts_utc"], errors="coerce", utc=True)

    # æ¥è‘—å°±èƒ½å®‰å…¨åšæ™‚é–“è·é›¢æ¯”è¼ƒ
    df["dt_diff"] = (df["ts_utc"] - t_ref).abs()

    tol = pd.Timedelta(minutes=tol_minutes)
    df = df[df["dt_diff"] <= tol].copy()
    if df.empty:
        return df

    # æ’åºï¼šåƒæ•¸ã€æ™‚é–“è·é›¢æœ€å°ã€æœ€æ–°æ™‚é–“ (ç¢ºä¿åŒä¸€åƒæ•¸åªç•™æœ€æ¥è¿‘ t_ref çš„é‚£ç­†)
    df = df.sort_values(["parameter", "dt_diff", "ts_utc"], ascending=[True, True, False])
    df = df.drop_duplicates(subset=["parameter"], keep="first")
    return df[["parameter", "value", "units", "ts_utc", "ts_local"]]


def fetch_latest_observation_data(location_id: int, target_params: list) -> pd.DataFrame:
    """
    ä¸»è¦æ•¸æ“šç²å–æµç¨‹ï¼š
    1. å–å¾—ç«™é»æœ€å¾Œæ›´æ–°æ™‚é–“ t_starã€‚
    2. å¾ /locations/{id}/latest å–å¾— df_loc_latestã€‚
    3. ä»¥ t_star ç‚ºåŸºæº–ï¼Œåœ¨ df_loc_latest ä¸­å°‹æ‰¾æœ€æ¥è¿‘ä¸”æ™‚é–“å°é½Šçš„ä¸€æ‰¹æ•¸æ“šã€‚
    4. é‡å°ç¼ºå°‘çš„åƒæ•¸ï¼Œå¾ /parameters/{pid}/latest è£œé½Šï¼Œä¸¦ä¹Ÿå°é½Š t_starã€‚
    5. åˆä½µçµæœï¼Œè¿”å›å–®è¡Œã€æ™‚é–“å°é½Šçš„ DataFrameã€‚
    """
    meta = get_location_meta(location_id)
    if not meta or pd.isna(meta["last_utc"]):
        print("ğŸš¨ [Fetch] ç„¡æ³•å–å¾—ç«™é»å…ƒæ•¸æ“šæˆ–æœ€å¾Œæ›´æ–°æ™‚é–“ã€‚")
        return pd.DataFrame()

    df_loc_latest = get_location_latest_df(location_id)
    if df_loc_latest.empty:
        print("âš ï¸ [Fetch] /locations/{id}/latest æ²’æœ‰ä»»ä½•è³‡æ–™ã€‚")
        return pd.DataFrame()

    # æ±ºå®šå°é½Šæ™‚é–“ t_star (ä½¿ç”¨ç«™é» meta æˆ– latest ä¸­çš„æœ€å¤§æ™‚é–“)
    t_star_latest = df_loc_latest["ts_utc"].max()
    t_star_loc = meta["last_utc"]
    t_star = t_star_latest if pd.notna(t_star_latest) else t_star_loc

    if pd.isna(t_star):
        print("ğŸš¨ [Fetch] ç„¡æ³•æ±ºå®šæœ‰æ•ˆçš„æ‰¹æ¬¡å°é½Šæ™‚é–“ã€‚")
        return pd.DataFrame()
    
    # 1. åœ¨ /locations/{id}/latest ä¸­æ‰¾ã€Œæ¥è¿‘ t_starã€çš„ä¸€æ‰¹
    df_at_batch = pick_batch_near(df_loc_latest, t_star, TOL_MINUTES_PRIMARY)
    if df_at_batch.empty:
        df_at_batch = pick_batch_near(df_loc_latest, t_star, TOL_MINUTES_FALLBACK)

    have = set(df_at_batch["parameter"].str.lower().tolist()) if not df_at_batch.empty else set()

    # 2. é‚„ç¼ºçš„åƒæ•¸ï¼Œç”¨ /parameters/{pid}/latest?locationId= è£œ
    missing = [p for p in target_params if p not in have]
    df_param_batch = pd.DataFrame()
    if missing:
        df_param_latest = get_parameters_latest_df(location_id, missing)
        df_param_batch = pick_batch_near(df_param_latest, t_star, TOL_MINUTES_PRIMARY)
        if df_param_batch.empty:
            df_param_batch = pick_batch_near(df_param_latest, t_star, TOL_MINUTES_FALLBACK)

    # 3. åˆä½µã€åªç•™ç›®æ¨™åƒæ•¸ã€å»é‡
    frames = [df for df in [df_at_batch, df_param_batch] if not df.empty]
    if not frames:
        print("âš ï¸ [Fetch] åœ¨æœ€å¾Œä¸€æ‰¹æ™‚é–“é™„è¿‘ï¼Œç›®æ¨™æ±¡æŸ“ç‰©éƒ½æ²’æœ‰è³‡æ–™ã€‚")
        return pd.DataFrame()

    df_all = pd.concat(frames, ignore_index=True)
    df_all["parameter"] = df_all["parameter"].str.lower()
    df_all = df_all[df_all["parameter"].isin(target_params)]

    # æœ€çµ‚å»é‡ (å–æœ€æ¥è¿‘ t_star çš„é‚£ç­†)
    df_all["dt_diff"] = (df_all["ts_utc"] - t_star).abs()
    df_all = df_all.sort_values(["parameter", "dt_diff", "ts_utc"], ascending=[True, True, False])
    df_all = df_all.drop_duplicates(subset=["parameter"], keep="first")
    df_all = df_all.drop(columns=["dt_diff", "units", "ts_local"]) # ç§»é™¤ä¸å¿…è¦çš„æ¬„ä½

    # 4. è½‰æ›æˆæ¨¡å‹è¼¸å…¥æ ¼å¼ (å–®è¡Œå¯¬è¡¨)
    observation = df_all.pivot_table(
        index='ts_utc', columns='parameter', values='value', aggfunc='first'
    ).reset_index()
    observation = observation.rename(columns={'ts_utc': 'datetime'})
    
    # è¨ˆç®— AQIï¼Œä¸¦ç¢ºä¿ column name ç‚º aqi
    observation['aqi'] = observation.apply(
        lambda row: calculate_aqi(row, target_params), axis=1
    )
    
    # ç¢ºä¿åªæœ‰ä¸€ç­†æ•¸æ“šï¼Œä¸¦ä¸”æ™‚é–“æ˜¯ UTC-aware
    if not observation.empty:
        observation['datetime'] = observation['datetime'].dt.tz_localize(None).dt.tz_localize('UTC')

    return observation


# =================================================================
# è¼”åŠ©å‡½å¼: AQI è¨ˆç®— (ä¿æŒä¸è®Š)
# =================================================================
# ... (calculate_aqi_sub_index ä¿æŒä¸è®Š) ...
def calculate_aqi_sub_index(param: str, concentration: float) -> float:
    """è¨ˆç®—å–®ä¸€æ±¡æŸ“ç‰©æ¿ƒåº¦å°æ‡‰çš„ AQI å­æŒ‡æ•¸ (I)"""
    if pd.isna(concentration) or concentration < 0:
        return 0

    breakpoints = AQI_BREAKPOINTS.get(param)
    if not breakpoints:
        return 0

    for C_low, C_high, I_low, I_high in breakpoints:
        if C_low <= concentration <= C_high:
            if C_high == C_low:
                return I_high
            I = ((I_high - I_low) / (C_high - C_low)) * (concentration - C_low) + I_low
            return np.round(I)

        if concentration > breakpoints[-1][1]:
            I_low, I_high = breakpoints[-1][2], breakpoints[-1][3]
            C_low, C_high = breakpoints[-1][0], breakpoints[-1][1]
            if C_high == C_low:
                return I_high
            I_rate = (I_high - I_low) / (C_high - C_low)
            I = I_high + I_rate * (concentration - C_high)
            return np.round(I)

    return 0

# ... (calculate_aqi ä¿æŒä¸è®Š) ...
def calculate_aqi(row: pd.Series, params: list) -> int:
    """æ ¹æ“šå¤šå€‹æ±¡æŸ“ç‰©æ¿ƒåº¦è¨ˆç®—æœ€çµ‚ AQI (å–æœ€å¤§å­æŒ‡æ•¸)"""
    sub_indices = []
    for p in params:
        col_name = f'{p}_pred' if f'{p}_pred' in row else p # æ³¨æ„é€™è£¡ p å³å¯ï¼Œå› ç‚ºæ˜¯é æ¸¬å‰çš„ raw value
        if col_name in row and not pd.isna(row[col_name]):
            sub_index = calculate_aqi_sub_index(p, row[col_name])
            sub_indices.append(sub_index)

    if not sub_indices:
        return np.nan

    return int(np.max(sub_indices))

# =================================================================
# é æ¸¬å‡½å¼ (ä¿æŒä¸è®Š)
# =================================================================

def predict_future_multi(models, last_data, feature_cols, pollutant_params, hours=24):
    """é æ¸¬æœªä¾† N å°æ™‚çš„å¤šå€‹ç›®æ¨™æ±¡æŸ“ç‰© (éè¿´é æ¸¬) ä¸¦è¨ˆç®— AQI"""
    predictions = []

    # last_data ç¾åœ¨æ˜¯å–®è¡Œ DataFrameï¼Œéœ€è¦å…ˆè½‰æ›æ™‚é–“æ ¼å¼
    last_data['datetime'] = pd.to_datetime(last_data['datetime']).dt.tz_localize('UTC')
    last_datetime_aware = last_data['datetime'].iloc[0]
    # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨ to_dict() å‰µå»ºä¸€å€‹å¯è®Šå­—å…¸å‰¯æœ¬ä½œç‚ºè¿­ä»£çš„åŸºç¤
    current_data_dict = last_data[feature_cols].iloc[0].to_dict() 

    weather_feature_names_base = ['temperature', 'humidity', 'pressure']
    weather_feature_names = [col for col in weather_feature_names_base if col in feature_cols]
    has_weather = bool(weather_feature_names)

    for h in range(hours):
        future_time = last_datetime_aware + timedelta(hours=h + 1)
        pred_features = current_data_dict.copy()

        # 1. æ›´æ–°æ™‚é–“ç‰¹å¾µ
        pred_features['hour'] = future_time.hour
        pred_features['day_of_week'] = future_time.dayofweek
        pred_features['month'] = future_time.month
        pred_features['day_of_year'] = future_time.timetuple().tm_yday
        pred_features['is_weekend'] = int(future_time.dayofweek in [5, 6])
        pred_features['hour_sin'] = np.sin(2 * np.pi * future_time.hour / 24)
        pred_features['hour_cos'] = np.cos(2 * np.pi * future_time.hour / 24)
        pred_features['day_sin'] = np.sin(2 * np.pi * pred_features['day_of_year'] / 365)
        pred_features['day_cos'] = np.cos(2 * np.pi * pred_features['day_of_year'] / 365)

        # 2. æ¨¡æ“¬æœªä¾†å¤©æ°£è®ŠåŒ– 
        if has_weather:
            np.random.seed(future_time.hour + future_time.day + 42)
            for w_col in weather_feature_names:
                base_value = current_data_dict.get(w_col)
                if base_value is not None and not np.isnan(base_value):
                    new_weather_value = base_value + np.random.normal(0, 0.5) 
                    pred_features[w_col] = new_weather_value
                    current_data_dict[w_col] = new_weather_value # æ›´æ–°ä»¥ä¾¿ä¸‹ä¸€å°æ™‚ä½¿ç”¨

        current_prediction_row = {'datetime': future_time}
        new_pollutant_values = {}

        # 3. é æ¸¬æ‰€æœ‰æ±¡æŸ“ç‰©
        for param in pollutant_params:
            model = models[param]
            pred_input = np.array([pred_features[col] for col in feature_cols]).reshape(1, -1)
            pred = model.predict(pred_input)[0]
            pred = max(0, pred)

            current_prediction_row[f'{param}_pred'] = pred
            new_pollutant_values[param] = pred

        # 4. è¨ˆç®—é æ¸¬çš„ AQI
        predicted_aqi = calculate_aqi(pd.Series(current_prediction_row), pollutant_params)
        current_prediction_row['aqi_pred'] = predicted_aqi
        new_pollutant_values['aqi'] = predicted_aqi

        predictions.append(current_prediction_row)

        # 5. æ›´æ–°æ»¯å¾Œç‰¹å¾µ
        for param in pollutant_params + ['aqi']:
            for i in range(len(LAG_HOURS) - 1, 0, -1):
                lag_current = LAG_HOURS[i]
                lag_prev = LAG_HOURS[i-1]
                lag_current_col = f'{param}_lag_{lag_current}h'
                lag_prev_col = f'{param}_lag_{lag_prev}h'

                if lag_current_col in current_data_dict and lag_prev_col in current_data_dict:
                    current_data_dict[lag_current_col] = current_data_dict[lag_prev_col]

            if f'{param}_lag_1h' in current_data_dict and param in new_pollutant_values:
                current_data_dict[f'{param}_lag_1h'] = new_pollutant_values[param]

    return pd.DataFrame(predictions)


# =================================================================
# æ¨¡å‹è¼‰å…¥é‚è¼¯ (ä¿æŒä¸è®Š)
# =================================================================
# ... (load_models_and_metadata ä¿æŒä¸è®Š) ...

def load_models_and_metadata():
    global TRAINED_MODELS, LAST_OBSERVATION, FEATURE_COLUMNS, POLLUTANT_PARAMS

    if not os.path.exists(META_PATH):
        print("ğŸš¨ [Load] æ‰¾ä¸åˆ°æ¨¡å‹å…ƒæ•¸æ“šæª”æ¡ˆ (model_meta.json)ï¼Œç„¡æ³•è¼‰å…¥æ¨¡å‹ã€‚")
        return

    try:
        # 1. è¼‰å…¥å…ƒæ•¸æ“š
        with open(META_PATH, 'r', encoding='utf-8') as f:
            metadata = json.load(f)

        POLLUTANT_PARAMS = metadata.get('pollutant_params', [])
        FEATURE_COLUMNS = metadata.get('feature_columns', [])
        
        # å°‡æœ€å¾Œä¸€ç­†æ•¸æ“šçš„ JSON è½‰æ›å› DataFrame
        if 'last_observation_json' in metadata:
            LAST_OBSERVATION = pd.read_json(metadata['last_observation_json'], orient='records')

        # 2. è¼‰å…¥ XGBoost æ¨¡å‹
        TRAINED_MODELS = {}
        for param in POLLUTANT_PARAMS:
            model_path = os.path.join(MODELS_DIR, f'{param}_model.json')
            if os.path.exists(model_path):
                model = xgb.XGBRegressor()
                model.load_model(model_path)
                TRAINED_MODELS[param] = model
            else:
                print(f"âŒ [Load] æ‰¾ä¸åˆ° {param} çš„æ¨¡å‹æª”æ¡ˆ: {model_path}")
                del POLLUTANT_PARAMS[POLLUTANT_PARAMS.index(param)]
        
        if TRAINED_MODELS:
            print(f"âœ… [Load] æˆåŠŸè¼‰å…¥ {len(TRAINED_MODELS)} å€‹æ¨¡å‹ã€‚")
        else:
            print("ğŸš¨ [Load] æœªè¼‰å…¥ä»»ä½•æ¨¡å‹ã€‚")


    except Exception as e:
        print(f"âŒ [Load] æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}") 
        TRAINED_MODELS = {} 
        LAST_OBSERVATION = None
        FEATURE_COLUMNS = []
        POLLUTANT_PARAMS = []

# =================================================================
# Flask æ‡‰ç”¨ç¨‹å¼è¨­å®šèˆ‡å•Ÿå‹•
# =================================================================
app = Flask(__name__)

with app.app_context():
    load_models_and_metadata() 

@app.route('/')
def index():
    city_name = "é«˜é›„"
    
    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦æˆåŠŸè¼‰å…¥
    if not TRAINED_MODELS or not POLLUTANT_PARAMS:
        print("ğŸš¨ [Request] æ¨¡å‹æˆ–åƒæ•¸å°šæœªåˆå§‹åŒ–ï¼Œç„¡æ³•é€²è¡Œé æ¸¬ã€‚")
        return render_template('index.html', max_aqi="N/A", aqi_predictions=[], city_name=city_name)
    
    # â­â­â­ æ–°å¢ï¼šå³æ™‚æŠ“å–æœ€æ–°è§€æ¸¬æ•¸æ“š â­â­â­
    current_observation_df = fetch_latest_observation_data(LOCATION_ID, POLLUTANT_TARGETS)

    if current_observation_df.empty or len(current_observation_df) == 0:
        print("ğŸš¨ [Request] ç„¡æ³•å–å¾—æœ€æ–°çš„ç©ºæ°£å“è³ªè§€æ¸¬æ•¸æ“šã€‚")
        # âš ï¸ å¯é¸ï¼šå¦‚æœæŠ“å–å¤±æ•—ï¼Œé€€å›åˆ°ä½¿ç”¨ LAST_OBSERVATION é€²è¡Œé æ¸¬
        observation_for_prediction = LAST_OBSERVATION
    else:
        observation_for_prediction = current_observation_df
        print(f"âœ… [Request] æˆåŠŸå–å¾—æœ€æ–°è§€æ¸¬æ•¸æ“š (UTC: {observation_for_prediction['datetime'].iloc[0]})")


    if observation_for_prediction is None or observation_for_prediction.empty:
        print("ğŸš¨ [Request] é æ¸¬æ•¸æ“šä¾†æºç‚ºç©ºï¼Œç„¡æ³•é€²è¡Œé æ¸¬ã€‚")
        return render_template('index.html', max_aqi="N/A", aqi_predictions=[], city_name=city_name)

    # å¿…é ˆç¢ºä¿ observation_for_prediction åŒ…å«æ‰€æœ‰ FEATURE_COLUMNS
    # é€™è£¡æˆ‘å€‘ä¿¡ä»»æ¨¡å‹è¨“ç·´æ™‚çš„é‚è¼¯ï¼Œå‡è¨­ç¼ºå¤±çš„æ•¸æ“šæœƒåœ¨æ¨¡å‹è¨“ç·´æ™‚è¢«è™•ç†æˆ Nan æˆ–å…¶ä»–é è¨­å€¼
    
    # â­â­â­ æ ¸å¿ƒé æ¸¬é‚è¼¯ (ä½¿ç”¨ observation_for_prediction) â­â­â­
    try:
        future_predictions = predict_future_multi(
            TRAINED_MODELS,
            observation_for_prediction, # ä½¿ç”¨æœ€æ–°æˆ–å‚™ç”¨æ•¸æ“š
            FEATURE_COLUMNS,
            POLLUTANT_PARAMS,
            hours=HOURS_TO_PREDICT
        )

        # æ ¼å¼åŒ–çµæœ
        future_predictions['datetime_local'] = future_predictions['datetime'].dt.tz_convert(LOCAL_TZ)
        max_aqi = int(future_predictions['aqi_pred'].max())

        aqi_predictions = [
            {'time': item['datetime_local'].strftime('%Y-%m-%d %H:%M'), 'aqi': int(item['aqi_pred'])}
            for item in future_predictions.to_dict(orient='records')
        ]
        
    except Exception as e:
        max_aqi = "N/A"
        aqi_predictions = []
        print(f"âŒ [Request] é æ¸¬åŸ·è¡Œå¤±æ•—: {e}") 

    return render_template('index.html', max_aqi=max_aqi, aqi_predictions=aqi_predictions, city_name=city_name)

if __name__ == '__main__':
    # åœ¨æœ¬åœ°ç’°å¢ƒé‹è¡Œæ™‚ä½¿ç”¨
    app.run(debug=True)
