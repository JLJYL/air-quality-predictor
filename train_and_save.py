# train_and_save.py - ä¾›æœ¬åœ°è¨“ç·´ä½¿ç”¨

# =================================================================
# å°å…¥æ‰€æœ‰å¿…è¦çš„åº«
# =================================================================
import requests
import pandas as pd
import datetime
import random
import re
import os
import warnings
import numpy as np
import xgboost as xgb
import json
from datetime import timedelta, timezone
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from meteostat import Point, Hourly, units

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore')

# å‰µå»ºä¸€å€‹ models è³‡æ–™å¤¾ä¾†å„²å­˜æ¨¡å‹
MODELS_DIR = 'models'
os.makedirs(MODELS_DIR, exist_ok=True)

# =================================================================
# è¤‡è£½ app.py ä¸­çš„å¸¸æ•¸è¨­å®š (å·²æ¸…ç†éæ¨™æº–å­—å…ƒ)
# =================================================================
API_KEY = "68af34aea77a19aa1137ee5fd9b287229ccf23a686309b4521924a04963ac663"
API_BASE_URL = "https://api.openaq.org/v3/"
POLLUTANT_TARGETS = ["pm25", "pm10", "o3", "no2", "so2", "co"]
LOCAL_TZ = "Asia/Taipei"
MIN_DATA_THRESHOLD = 100
LAG_HOURS = [1, 2, 3, 6, 12, 24]
ROLLING_WINDOWS = [6, 12, 24]
# é€™è£¡ä½¿ç”¨ 30 å¤©æ•¸æ“šé€²è¡Œæ›´ç©©å®šçš„è¨“ç·´ï¼Œå› ç‚ºåœ¨æœ¬åœ°é‹è¡Œæ²’æœ‰è¶…æ™‚é¡§æ…®
DAYS_TO_FETCH = 30
# æ¨¡å‹è¨“ç·´åƒæ•¸ï¼šä½¿ç”¨åŸå§‹è¼ƒé«˜çš„ N_ESTIMATORS ç²å¾—æ›´å¥½çš„æ¨¡å‹
N_ESTIMATORS = 150

# ç°¡åŒ–çš„ AQI åˆ†ç´šè¡¨ (åŸºæ–¼å°æ™‚å€¼å’Œ US EPA æ¨™æº–çš„å¸¸ç”¨æ•¸å€¼)
AQI_BREAKPOINTS = {
    "pm25": [(0.0, 12.0, 0, 50), (12.1, 35.4, 51, 100), (35.5, 55.4, 101, 150), (55.5, 150.4, 151, 200)],
    "pm10": [(0, 54, 0, 50), (55, 154, 51, 100), (155, 254, 101, 150), (255, 354, 151, 200)],
    "o3": [(0, 54, 0, 50), (55, 70, 51, 100), (71, 85, 101, 150), (86, 105, 151, 200)],
    "co": [(0.0, 4.4, 0, 50), (4.5, 9.4, 51, 100), (9.5, 12.4, 101, 150), (12.5, 15.4, 151, 200)],
    "no2": [(0, 100, 0, 50), (101, 360, 51, 100), (361, 649, 101, 150), (650, 1249, 151, 200)],
    "so2": [(0, 35, 0, 50), (36, 75, 51, 100), (76, 185, 101, 150), (186, 304, 151, 200)],
}

# =================================================================
# AQI è¼”åŠ©å‡½å¼
# =================================================================
def calculate_aqi_sub_index(param: str, concentration: float) -> float:
    """è¨ˆç®—å–®ä¸€æ±¡æŸ“ç‰©æ¿ƒåº¦å°æ‡‰çš„ AQI å­æŒ‡æ•¸ (I)"""
    if pd.isna(concentration) or concentration < 0:
        return 0

    breakpoints = AQI_BREAKPOINTS.get(param)
    if not breakpoints:
        return 0

    for C_low, C_high, I_low, I_high in breakpoints:
        if C_low <= concentration <= C_high:
            if C_high == C_low:
                return I_high
            I = ((I_high - I_low) / (C_high - C_low)) * (concentration - C_low) + I_low
            return np.round(I)

        if concentration > breakpoints[-1][1]:
            I_low, I_high = breakpoints[-1][2], breakpoints[-1][3]
            C_low, C_high = breakpoints[-1][0], breakpoints[-1][1]
            if C_high == C_low:
                return I_high
            I_rate = (I_high - I_low) / (C_high - C_low)
            I = I_high + I_rate * (concentration - C_high)
            return np.round(I)

    return 0

def calculate_aqi(row: pd.Series, params: list) -> int:
    """æ ¹æ“šå¤šå€‹æ±¡æŸ“ç‰©æ¿ƒåº¦è¨ˆç®—æœ€çµ‚ AQI (å–æœ€å¤§å­æŒ‡æ•¸)"""
    sub_indices = []
    for p in params:
        col_name = f'{p}_pred' if f'{p}_pred' in row else f'{p}_value'
        if col_name in row and not pd.isna(row[col_name]):
            sub_index = calculate_aqi_sub_index(p, row[col_name])
            sub_indices.append(sub_index)

    if not sub_indices:
        return np.nan

    return int(np.max(sub_indices))


# =================================================================
# OpenAQ è¼”åŠ©å‡½å¼
# =================================================================
def sanitize_filename(name: str) -> str:
    return re.sub(r'[\\/:"*?<>|]+', '_', name)

def get_nearest_station(lat, lon, radius=20000, limit=50, days=7):
    """ æ‰¾é›¢ (lat,lon) æœ€è¿‘ä¸”æœ€è¿‘ days å…§æœ‰æ›´æ–°çš„æ¸¬ç«™ """
    url = f"{API_BASE_URL}locations"
    headers = {"X-API-Key": API_KEY}
    params = {"coordinates": f"{lat},{lon}", "radius": radius, "limit": limit}
    try:
        resp = requests.get(url, headers=headers, params=params)
        resp.raise_for_status()
        j = resp.json()
    except Exception as e:
        print(f"Error fetching nearest station: {e}")
        return None

    if "results" not in j or not j["results"]:
        return None

    df = pd.json_normalize(j["results"])
    if "datetimeLast.utc" not in df.columns:
        return None

    df["datetimeLast.utc"] = pd.to_datetime(df["datetimeLast.utc"], errors="coerce", utc=True)
    now = pd.Timestamp.utcnow()
    cutoff = now - pd.Timedelta(days=days)
    df = df[(df["datetimeLast.utc"] >= cutoff) & (df["datetimeLast.utc"] <= now)]
    if df.empty:
        return None

    nearest = df.sort_values("distance").iloc[0]
    return nearest.to_dict()

def get_station_sensors(station_id):
    """ ä½¿ç”¨ /locations/{id}/sensors å–å¾— sensors åˆ—è¡¨ """
    url = f"{API_BASE_URL}locations/{station_id}/sensors"
    headers = {"X-API-Key": API_KEY}
    try:
        resp = requests.get(url, headers=headers, params={"limit":1000})
        resp.raise_for_status()
        j = resp.json()
        return j.get("results", [])
    except Exception as e:
        print(f"Error fetching sensors: {e}")
        return []

def _extract_datetime_from_measurement(item: dict):
    """ å˜—è©¦å¾ measurement ç‰©ä»¶æŠ½å‡ºæ™‚é–“å­—ä¸² """
    candidates = [("period", "datetimeFrom", "utc"), ("date", "utc"), ("datetime",)]
    for path in candidates:
        cur = item
        ok = True
        for k in path:
            if isinstance(cur, dict) and k in cur:
                cur = cur[k]
            else:
                ok = False
                break
        if ok and cur:
            return cur
    return None

def fetch_sensor_data(sensor_id, param_name, limit=500, days=7):
    """ æ“·å– sensor çš„æ™‚é–“åºåˆ— """
    url = f"{API_BASE_URL}sensors/{sensor_id}/measurements"
    headers = {"X-API-Key": API_KEY}
    now = datetime.datetime.now(datetime.timezone.utc)
    date_from = (now - datetime.timedelta(days=days)).isoformat().replace("+00:00", "Z")
    params = {"limit": limit, "date_from": date_from}

    try:
        resp = requests.get(url, headers=headers, params=params)
        resp.raise_for_status()
        j = resp.json()
        results = j.get("results", [])
    except Exception as e:
        print(f"âŒ æŠ“å– {param_name} æ•¸æ“šå¤±æ•—: {e}")
        return pd.DataFrame()

    rows = []
    for r in results:
        dt_str = _extract_datetime_from_measurement(r)
        try:
            ts = pd.to_datetime(dt_str, utc=True)
        except Exception:
            ts = pd.NaT
        rows.append({"datetime": ts, param_name: r.get("value")})

    df = pd.DataFrame(rows).dropna(subset=["datetime"])
    if df.empty:
        return pd.DataFrame()
    df = df.sort_values("datetime", ascending=False).drop_duplicates(subset=["datetime"])
    return df

def generate_fake_data(limit=10, params=POLLUTANT_TARGETS):
    """ç”Ÿæˆæ‰€æœ‰ç›®æ¨™æ±¡æŸ“ç‰© (å« AQI) çš„æ¨¡æ“¬æ•¸æ“š"""
    now = datetime.datetime.now(datetime.timezone.utc)
    base_rows = []
    for i in range(limit):
        dt = now - datetime.timedelta(minutes=i*60)
        row = {'datetime': dt}

        for param in params:
            if param in ["pm25", "pm10"]: value = round(random.uniform(10, 60), 1)
            elif param == "o3": value = round(random.uniform(20, 100), 1)
            elif param in ["no2", "so2"]: value = round(random.uniform(1, 40), 1)
            elif param == 'co': value = round(random.uniform(0.1, 5), 1)
            row[f'{param}_value'] = value

        row['temperature'] = round(random.uniform(15, 30), 1)
        row['humidity'] = round(random.uniform(50, 95), 1)
        row['pressure'] = round(random.uniform(1000, 1020), 1)

        aqi_val = calculate_aqi(pd.Series(row), params)
        row['aqi_value'] = aqi_val
        base_rows.append(row)

    df = pd.DataFrame(base_rows)
    df['datetime'] = df['datetime'].dt.tz_localize('UTC')
    return df

def get_all_target_data(station_id, target_params, days_to_fetch):
    """ç²å–æ‰€æœ‰ç›®æ¨™æ±¡æŸ“ç‰©æ•¸æ“šä¸¦åˆä½µ"""
    sensors = get_station_sensors(station_id)
    sensor_map = {s.get("parameter", {}).get("name", "").lower(): s.get("id") for s in sensors}

    all_dfs = []
    found_params = []

    for param in target_params:
        sensor_id = sensor_map.get(param)
        if sensor_id:
            # ç”±æ–¼ OpenAQ v3 API çš„é™åˆ¶ï¼Œæ¯æ¬¡å‘¼å«æœ€å¤šåªèƒ½å– 500 ç­†è³‡æ–™
            df_param = fetch_sensor_data(sensor_id, param, days=days_to_fetch)
            if not df_param.empty:
                df_param.rename(columns={param: f'{param}_value'}, inplace=True)
                all_dfs.append(df_param)
                found_params.append(param)

    if not all_dfs:
        return pd.DataFrame(), []

    merged_df = all_dfs[0]
    for i in range(1, len(all_dfs)):
        merged_df = pd.merge(merged_df, all_dfs[i], on='datetime', how='outer')

    return merged_df, found_params

# =================================================================
# WeatherCrawler é¡
# =================================================================
class WeatherCrawler:
    """Meteostat å°æ™‚ç´šå¤©æ°£æ•¸æ“šçˆ¬èŸ²èˆ‡æ•´åˆ"""

    def __init__(self, lat, lon):
        self.point = Point(lat, lon)
        self.weather_cols = {
            'temp': 'temperature',
            'rhum': 'humidity',
            'pres': 'pressure',
        }

    def fetch_and_merge_weather(self, air_quality_df: pd.DataFrame):
        """æ ¹æ“šç©ºæ°£å“è³ªæ•¸æ“šçš„æ™‚é–“ç¯„åœï¼Œå¾ Meteostat ç²å–å°æ™‚ç´šå¤©æ°£æ•¸æ“šä¸¦åˆä½µã€‚"""
        if air_quality_df.empty:
            return air_quality_df

        if air_quality_df['datetime'].dt.tz is None:
            air_quality_df['datetime'] = air_quality_df['datetime'].dt.tz_localize('UTC')

        start_time_utc_aware = air_quality_df['datetime'].min()
        end_time_utc_aware = air_quality_df['datetime'].max()

        # Meteostat æœŸæœ›ç„¡æ™‚å€çš„ datetime ç‰©ä»¶
        start_dt = start_time_utc_aware.tz_convert(None).to_pydatetime()
        end_dt = end_time_utc_aware.tz_convert(None).to_pydatetime()

        try:
            data = Hourly(self.point, start_dt, end_dt)
            weather_data = data.fetch()
        except Exception as e:
            print(f"âŒ æŠ“å– Meteostat æ•¸æ“šå¤±æ•—: {e}")
            weather_data = pd.DataFrame()

        if weather_data.empty:
            # å¦‚æœæŠ“å–å¤±æ•—ï¼Œå‰‡å¡«å…… NaN
            empty_weather = pd.DataFrame({'datetime': air_quality_df['datetime'].unique()})
            for col in self.weather_cols.values():
                empty_weather[col] = np.nan
            return pd.merge(air_quality_df, empty_weather, on='datetime', how='left')

        weather_data = weather_data.reset_index()
        weather_data.rename(columns={'time': 'datetime'}, inplace=True)
        weather_data = weather_data.rename(columns=self.weather_cols)
        weather_data = weather_data[list(self.weather_cols.values()) + ['datetime']]
        weather_data['datetime'] = weather_data['datetime'].dt.tz_localize('UTC')

        merged_df = pd.merge(
            air_quality_df,
            weather_data,
            on='datetime',
            how='left'
        )

        weather_cols_list = list(self.weather_cols.values())
        # ä½¿ç”¨ ffill/bfill è™•ç†ç¼ºå¤±å¤©æ°£æ•¸æ“š
        merged_df[weather_cols_list] = merged_df[weather_cols_list].fillna(method='ffill').fillna(method='bfill')

        return merged_df

    def get_weather_feature_names(self):
        return list(self.weather_cols.values())

# =================================================================
# è¨“ç·´èˆ‡å„²å­˜æ¨¡å‹çš„é‚è¼¯
# =================================================================
def train_and_save_models(lat: float, lon: float, days_to_fetch: int):
    print(f"ğŸ”¥ [Local Init] é–‹å§‹åŸ·è¡Œæœ¬åœ° AQI é æ¸¬åˆå§‹åŒ–æµç¨‹ (æ•¸æ“šé‡: {days_to_fetch} å¤©)...")

    weather = WeatherCrawler(lat, lon)

    try:
        # 1. æ•¸æ“šæ”¶é›†
        station = get_nearest_station(lat, lon, days=days_to_fetch)

        if not station:
            print("ğŸš¨ [Local Init] æœªæ‰¾åˆ°æ´»èºæ¸¬ç«™ï¼Œä½¿ç”¨æ¨¡æ“¬æ•¸æ“šã€‚")
            df = generate_fake_data(limit=days_to_fetch * 24, params=POLLUTANT_TARGETS)
            found_target_params = POLLUTANT_TARGETS
        else:
            print(f"âœ… [Local Init] æ‰¾åˆ°æ¸¬ç«™: {station['name']} ({station['id']})")
            df_raw, found_target_params = get_all_target_data(station["id"], POLLUTANT_TARGETS, days_to_fetch)

            if df_raw.empty or len(df_raw) < MIN_DATA_THRESHOLD:
                print("ğŸš¨ [Local Init] å¯¦éš›æ•¸æ“šé‡ä¸è¶³ï¼Œä½¿ç”¨æ¨¡æ“¬æ•¸æ“šã€‚")
                df = generate_fake_data(limit=days_to_fetch * 24, params=POLLUTANT_TARGETS)
                found_target_params = POLLUTANT_TARGETS
            else:
                df = df_raw.copy()

            # åˆä½µ Meteostat å¤©æ°£æ•¸æ“š
            df = weather.fetch_and_merge_weather(df)

        POLLUTANT_PARAMS_TRAINED = found_target_params
        weather_feature_names = weather.get_weather_feature_names()
        value_cols = [f'{p}_value' for p in POLLUTANT_PARAMS_TRAINED]
        all_data_cols = value_cols + weather_feature_names

        # é‡æ¡æ¨£åˆ°å°æ™‚
        df.set_index('datetime', inplace=True)
        df = df[value_cols + weather_feature_names].resample('H').mean()
        df.reset_index(inplace=True)
        df = df.dropna(how='all', subset=all_data_cols)

        # è¨ˆç®—æ­·å² AQI
        df['aqi_value'] = df.apply(lambda row: calculate_aqi(row, POLLUTANT_PARAMS_TRAINED), axis=1)

        # ç§»é™¤ä»»ä¸€æ±¡æŸ“ç‰©æˆ–å¤©æ°£æ•¸æ“šç‚º NaN çš„è¡Œ (ç¢ºä¿æ¨¡å‹è¼¸å…¥å®Œæ•´)
        df = df.dropna(subset=all_data_cols + ['aqi_value']).reset_index(drop=True)
        print(f"ğŸ“Š [Local Init] æœ€çµ‚ç”¨æ–¼è¨“ç·´çš„æ•¸æ“šé‡: {len(df)} å°æ™‚")


        if len(df) <= max(LAG_HOURS):
            raise ValueError(f"æœ€çµ‚æ•¸æ“šé‡ ({len(df)}) ä¸è¶³ {max(LAG_HOURS)}ï¼Œç„¡æ³•é€²è¡Œæ»¯å¾Œç‰¹å¾µå·¥ç¨‹å’Œè¨“ç·´ã€‚")


        # 2. ç‰¹å¾µå·¥ç¨‹
        df['hour'] = df['datetime'].dt.hour
        df['day_of_week'] = df['datetime'].dt.dayofweek
        df['month'] = df['datetime'].dt.month
        df['day_of_year'] = df['datetime'].dt.dayofyear
        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
        df['day_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)
        df['day_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)

        df = df.sort_values('datetime')
        feature_base_cols = value_cols + ['aqi_value']

        for col_name in feature_base_cols:
            param = col_name.replace('_value', '')
            for lag in LAG_HOURS:
                df[f'{param}_lag_{lag}h'] = df[col_name].shift(lag)

            if 'aqi' not in param:
                for window in ROLLING_WINDOWS:
                    df[f'{param}_rolling_mean_{window}h'] = df[col_name].rolling(window=window, min_periods=1).mean()
                    df[f'{param}_rolling_std_{window}h'] = df[col_name].rolling(window=window, min_periods=1).std()

        df = df.dropna().reset_index(drop=True)

        # å„²å­˜æœ€å¾Œä¸€ç­†æ•¸æ“šï¼Œç”¨æ–¼æœªä¾†é æ¸¬çš„èµ·é»
        LAST_OBSERVATION = df.iloc[-1:].to_json(orient='records', date_format='iso') # åºåˆ—åŒ–ç‚º JSON

        base_time_features = ['hour', 'day_of_week', 'month', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos']

        air_quality_features = []
        for param in POLLUTANT_PARAMS_TRAINED + ['aqi']:
            for lag in LAG_HOURS:
                air_quality_features.append(f'{param}_lag_{lag}h')
            if param != 'aqi':
                for window in ROLLING_WINDOWS:
                    air_quality_features.append(f'{param}_rolling_mean_{window}h')
                    air_quality_features.append(f'{param}_rolling_std_{window}h')

        FEATURE_COLUMNS = weather_feature_names + base_time_features + air_quality_features
        FEATURE_COLUMNS = [col for col in FEATURE_COLUMNS if col in df.columns]

        # 3. æ•¸æ“šåˆ†å‰²èˆ‡æ¨¡å‹è¨“ç·´ (80% è¨“ç·´)
        split_idx = int(len(df) * 0.8)
        X = df[FEATURE_COLUMNS]
        Y = {param: df[f'{param}_value'] for param in POLLUTANT_PARAMS_TRAINED}

        X_train = X[:split_idx]
        Y_train = {param: Y[param][:split_idx] for param in POLLUTANT_PARAMS_TRAINED}

        print(f"â³ [Local Init] é–‹å§‹è¨“ç·´ {len(POLLUTANT_PARAMS_TRAINED)} å€‹ XGBoost æ¨¡å‹ (N={N_ESTIMATORS})...")
        TRAINED_MODELS = {}
        for param in POLLUTANT_PARAMS_TRAINED:
            print(f"       è¨“ç·´ {param} æ¨¡å‹...")
            xgb_model = xgb.XGBRegressor(
                n_estimators=N_ESTIMATORS, max_depth=7, learning_rate=0.08, random_state=42, n_jobs=-1
            )
            xgb_model.fit(X_train, Y_train[param])
            TRAINED_MODELS[param] = xgb_model
            # å„²å­˜æ¨¡å‹
            model_path = os.path.join(MODELS_DIR, f'{param}_model.json')
            # ä½¿ç”¨ save_model() å„²å­˜ JSON æ ¼å¼
            xgb_model.save_model(model_path)
            print(f"       âœ… {param} æ¨¡å‹å·²å„²å­˜è‡³ {model_path}")

        # å„²å­˜æ¨¡å‹å…ƒæ•¸æ“š (Metadata)
        metadata = {
            'pollutant_params': POLLUTANT_PARAMS_TRAINED,
            'feature_columns': FEATURE_COLUMNS,
            'last_observation_json': LAST_OBSERVATION
        }
        with open(os.path.join(MODELS_DIR, 'model_meta.json'), 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=4)

        print("âœ… [Local Init] æ‰€æœ‰æ¨¡å‹å’Œå…ƒæ•¸æ“šå„²å­˜å®Œæˆã€‚")

    except Exception as e:
        print(f"âŒ [Local Init] è¨“ç·´åŸ·è¡Œå¤±æ•—: {e}")

if __name__ == '__main__':
    LAT, LON = 22.6273, 120.3014 # é«˜é›„
    train_and_save_models(LAT, LON, DAYS_TO_FETCH)